

hadoop@ubuntu:~$ cat > samp3.txt
70,90,50,pass
60,70,80,pass
20,30,90,fail
70,90,80,pass
10,30,70,fail
45,55,75,pass
15,70,20,fail           
90,20,70,fail       
70,80,65,passhadoop@ubuntu:~$ 
hadoop@ubuntu:~$ hdfs dfs -put samp3.txt /pyspark630pm

> from numpy import array
>>> from pyspark.mllib.linalg import Vector
>>> from pyspark.mllib.regression import LabeledPoint
>>> from pyspark.mllib.classification import LogisticRegressionWithSGD

step 1:Loading Data
>>> data=sc.textFile("hdfs://localhost:9000/pyspark630pm/samp3.txt")
>>> data.collect()
[u'70,90,50,pass', u'60,70,80,pass', u'20,30,90,fail', u'70,90,80,pass', u'10,30,70,fail', u'45,55,75,pass', u'15,70,20,fail           ', u'90,20,70,fail       ', u'70,80,65,pass']

step 2: Splitting and data preparation
        label should be float, so transform "pass" as 1.0 and fail as 0.0
 
>>> split1=data.map(lambda x:x.split(","))
>>> split1.collect()
[[u'70', u'90', u'50', u'pass'], [u'60', u'70', u'80', u'pass'], [u'20', u'30', u'90', u'fail'], [u'70', u'90', u'80', u'pass'], [u'10', u'30', u'70', u'fail'], [u'45', u'55', u'75', u'pass'], [u'15', u'70', u'20', u'fail           '], [u'90', u'20', u'70', u'fail       '], [u'70', u'80', u'65', u'pass']]


>>> def extract(x):
...    if(x[3]=="pass"):
...       return 1
...    else:
...       return 0
... 

>>> s2=split1.map(lambda x:(float(x[0]),float(x[1]),float(x[2]),float(extract(x))))
>>> s2.collect()
[(70.0, 90.0, 50.0, 1.0), (60.0, 70.0, 80.0, 1.0), (20.0, 30.0, 90.0, 0.0), (70.0, 90.0, 80.0, 1.0), (10.0, 30.0, 70.0, 0.0), (45.0, 55.0, 75.0, 1.0), (15.0, 70.0, 20.0, 0.0), (90.0, 20.0, 70.0, 0.0), (70.0, 80.0, 65.0, 1.0)]


step 3: Transforming into LabeledPoint
>>> lp=s2.map(lambda x:LabeledPoint(x[3],array([x[0],x[1],x[2]])))

step 4: now Building the model
>>> numIterations=100
>>> model1=LogisticRegressionWithSGD.train(lp,numIterations,0.0001)

step 5: now applying the model on training examples and predicting
... 
>>> testres1=lp.map(lambda x:(x.label,model1.predict(x.features)))
>>> testres1.collect()
[(1.0, 1), (1.0, 1), (0.0, 1), (1.0, 1), (0.0, 1), (1.0, 1), (0.0, 1), (0.0, 1), (1.0, 1)]


step 6: Testing the Accuracy (either pass/fail)
... 
>>> 
>>> def statistics(x):
...    if(x[0]==x[1]):
...       return "PASS"
...    else:
...       return "FAIL"
... 
>>> teststatistics=testres1.map(lambda x:statistics(x))
>>> teststatistics.collect()
['PASS', 'PASS', 'FAIL', 'PASS', 'FAIL', 'PASS', 'FAIL', 'FAIL', 'PASS']


step 7: how many tests passed and how many failed
... 
>>> teststatistics.countByValue()
defaultdict(<type 'int'>, {'FAIL': 4, 'PASS': 5})
