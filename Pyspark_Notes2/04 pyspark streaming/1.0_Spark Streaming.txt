Spark Streaming:
---------------
Spark Streaming is used to stream data  from various sources.

The sources can a file(LFS/HDFS) or network port or remote host(twitter ,facebook etc..)
 or from any other application or messaging systems such as Kafka,JMS etc..

but directly some applications cannot communicate with spark streaming,so at that time we can 
take help of messaging systems like KAfka,JMS etc

Purpose of Spark Streaming:  Micro-batching
---------------------------
Spark Streaming streams data from the sources and batches them(mini batches) and
peform micro-batch Analytics

Spark core dataobjects are called RDD's
spark sql data objects are called as DataFrames
Spark Streaming Data objects are called DSstreams

DSstreams means Descritized Data Streams

DSstreams is a continuous series of RDDs

Micro batching operation will be applied on each RDD of Dsstream

For every given period of interval,These RDDs will be build under one DSstream

Microbatching period is 10sec
i.e for every 10 secs  one RDD will be produced under one Dsstream.

As Streaming job is running, these RDDS will be generating continuously....

These independent RDDS will be processed by spark core.


here Each RDD is called a batch

who prepared this batch------------->Spark Streaming

who will process this batch--------->Spark core

Various Contexts:
Spark core--------->SparkContext(sc)

Sparksql ---------->sqlContext

sparkhql----------->hive context(hc)

sparkstreaming----->spak streaming context(ssc)

------------------------------------------------------------------------------------------

SparkStreamingContext(ssc) is used to create DSstreams

ex: 
  ssc=SparkStreamingContext(sc,10)
                                   here 10 is microbatch period

for every 10seconds worth of streamed data will be buffered at some worker node
of spark cluster and prepared as batch(RDD of Dsstream).

once Batch(RDD) is prepared, that will be submitted to spark core

At one side Spark core is processing the batch,
at the other side sparkstreaming keeps generating the batches(RDDS)
i.e keeps collecting the data from sources and prepares the next batches.


keeping batch period as 1 hour---------->its a Bad Idea

1 hour means Batch,not micro-batch
micro-batch means 1 or 2 mins

for bigger organizations----->microbatch is set to 10secs only


streaming responsibility is creating micro batches only
but spark streaming wont perform micro-batch analytics

preparing microbatches--->that too in the form of Dsstreams--->Dsstream is continuos
series of RDDS

we apply a algorithm on a DSstream,automatically it will happen on every RDD ofDSstream


















































