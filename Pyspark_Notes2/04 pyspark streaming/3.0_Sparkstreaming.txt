ssc=streamingContext(sc,10) #here 10 is micro-batch period

ex: streaming from a file

   ds1=ssc.textFileStream()
            here i can directly access the file data

       ssc.socketTextStream("localhost",9999) #here host is localhost
                                               port is ---->9999

       if  i start any port with 9999 like netcat etc.
       always my streamingcontext will be listening and capturing fron that port

whenever one event is genersted at that port---->Automatically it will be captured by
socketTextStream

It will be captured but immediately it wont be passed to spark core
when it will be passed---->after 10secs
but within this 10secs----->assume 100 events are generated from the given port---->formed
as one batch

once 10secs----->completed---->one batch is formed till then spark streaming keeps
buffering the events(ex:100 events)

now it is passed to spark core and processed

1st event---->I like spark
2nd event---->you like hadoop
3rd event---->he likes python

all these 3 events are kept in one RDD1 of DS1

Task:Simple wordcount

here we need to work with ds1
here we doesnt have direct acess with RDD1

first Faltten the things

here 3 recs---->3strings
  
code:
-----
lines(ds1)=ssc.socketTextStream("localhost",9999) #here lines is a dsstream
words=lines(ds1).flatMap(x=>x.split(" "))
o/p: [I,like,spark,you like hadoop,he likes python]

As spark core is processing 
Spark streaming will be preparing batches continuously

Note: Transformation over dsstream returns dsstream only...

pair=words.map(lambda word:(word,1))

words_count=pairs.reduceByKey(lambda x,y:x+y)

words_count.ppprint()

ssc.start()
---------------------------------------------------------------------------------------------
Total program code:

import findspark
findspark.init()
findspark.find()

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
sc=SparkContext("local[2]","NetworkWordCount")
ssc=StreamingContext(sc,3)
lines=ssc.socketTextStream("localhost",9999)
words=lines.flatMap(lambda line:line.split(" "))
pairs=words.map(lambda word:(word,1))
words_count=pairs.reduceByKey(lambda x,y:x+y)
words_count.pprint()
ssc.start()

start a network port
open terminal1:
$ nc -lk 9999
I Love Spark
You Love Hadoop
She Love Python
he Love spark and python

