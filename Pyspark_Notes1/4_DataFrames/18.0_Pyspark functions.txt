Pyspark Functions:
------------------
Column Transformations
1)withColumn()
  we can perform the following
  i)for changing the column datatype
 ii)for modifying or updating column value
iii)Deriving new column from the existing column
 iv)Renaming a column
  v)dropping a dataframe column

>>> data=[(101,'Miller',50000,'2020-02-21','M','pune'),
       (102,'Blake',60000,'2019-04-15','M','hyd'),
       (103,'Priya',30000,'2021-05-22','F','pune'),
       (104,'Sony',70000,'2018-05-22','F','hyd')]
>>> columns=["Empid","Empname","Salary","JoinDate","Sex","City"]
>>> df=spark.createDataFrame(data,columns)

>>> df.show()
+-----+-------+------+----------+---+----+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+


i)Changing the column datatype
  for changing the datatype,we use cast() function along with withColumn()

>>> df.printSchema()
root
 |-- Empid: long (nullable = true)
 |-- Empnmae: string (nullable = true)
 |-- Salary: long (nullable = true)
 |-- JoinDate: string (nullable = true)
 |-- Sex: string (nullable = true)
 |-- City: string (nullable = true)

>>> #Changing the salary column datatype
... 
>>> from pyspark.sql.functions import col
>>> df1=df.withColumn("Salary",col("Salary").cast("Integer"))
>>> df1.show()
+-----+-------+------+----------+---+----+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> df1.printSchema()
root
 |-- Empid: long (nullable = true)
 |-- Empnmae: string (nullable = true)
 |-- Salary: integer (nullable = true)
 |-- JoinDate: string (nullable = true)
 |-- Sex: string (nullable = true)
 |-- City: string (nullable = true)
-----------------------------------------------------------------------------------
ii)modifying or updating column value
   i)updating with a value
  ii)updating based on condition

Task: incrementing the salaries with 20% hike
>>> df2=df.withColumn("Salary",df.Salary+df.Salary*0.20)
>>> df2.show()
+-----+-------+-------+----------+---+----+
|Empid|Empnmae| Salary|  JoinDate|Sex|City|
+-----+-------+-------+----------+---+----+
|  101| Miller|60000.0|2020-02-21|  M|pune|
|  102|  Blake|72000.0|2019-04-15|  M| hyd|
|  103|  Priya|36000.0|2021-05-22|  F|pune|
|  104|   Sony|84000.0|2018-05-22|  F| hyd|
+-----+-------+-------+----------+---+----+

#adding 5000 as bonus to each employee

>>> df3=df.withColumn("Salary",df.Salary+5000)
>>> df3.show()
+-----+-------+------+----------+---+----+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 55000|2020-02-21|  M|pune|
|  102|  Blake| 65000|2019-04-15|  M| hyd|
|  103|  Priya| 35000|2021-05-22|  F|pune|
|  104|   Sony| 75000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

ii)updating based on condition
   -here we use withColumn() along with when
    modify------>"M"------>"MALE"
                 "F"------>"FEMALE"

>>> from pyspark.sql.functions import when
>>> df4=df.withColumn("Sex",when(df.Sex=="M","MALE") \
...      .when(df.Sex=="F","FEMALE") \
...     .otherwise(df.Sex))
>>> df4.show()
+-----+-------+------+----------+------+----+
|Empid|Empnmae|Salary|  JoinDate|   Sex|City|
+-----+-------+------+----------+------+----+
|  101| Miller| 50000|2020-02-21|  MALE|pune|
|  102|  Blake| 60000|2019-04-15|  MALE| hyd|
|  103|  Priya| 30000|2021-05-22|FEMALE|pune|
|  104|   Sony| 70000|2018-05-22|FEMALE| hyd|
+-----+-------+------+----------+------+----+


----------------------------------------------------------------------------------
3)Adding new column:
  -Adding new column with default/constant/None/Null value
  -Adding new column based on another column
  -Adding column based on condition

i)Adding new column with constant
  -here we use lit() function

#adding a new column hike_percent
>>> from pyspark.sql.functions import lit
>>> df5=df.withColumn("Hike_percent",lit(0.30))
>>> df5.show()
+-----+-------+------+----------+---+----+------------+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|Hike_percent|
+-----+-------+------+----------+---+----+------------+
|  101| Miller| 50000|2020-02-21|  M|pune|         0.3|
|  102|  Blake| 60000|2019-04-15|  M| hyd|         0.3|
|  103|  Priya| 30000|2021-05-22|  F|pune|         0.3|
|  104|   Sony| 70000|2018-05-22|  F| hyd|         0.3|
+-----+-------+------+----------+---+----+------------+

#Note: If you want to add NULL/None---->then use lit(None)

ii)Adding column based on another column of a dataframe
  Generate 2 columns---->Tax and netsal based on existing column Salary

>>> df5=df.withColumn("Tax",df.Salary*0.10)
>>> df5=df5.withColumn("NetSalary",df5.Salary-df5.Tax)
>>> df5.show()
+-----+-------+------+----------+---+----+------+---------+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|   Tax|NetSalary|
+-----+-------+------+----------+---+----+------+---------+
|  101| Miller| 50000|2020-02-21|  M|pune|5000.0|  45000.0|
|  102|  Blake| 60000|2019-04-15|  M| hyd|6000.0|  54000.0|
|  103|  Priya| 30000|2021-05-22|  F|pune|3000.0|  27000.0|
|  104|   Sony| 70000|2018-05-22|  F| hyd|7000.0|  63000.0|
+-----+-------+------+----------+---+----+------+---------+

iii)Adding column by concatenation existing columns with seperator

>>> df6=df.withColumn("Empid Empname",concat_ws(" ","Empid","Empnmae"))
>>> df6.show()
+-----+-------+------+----------+---+----+-------------+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|Empid Empname|
+-----+-------+------+----------+---+----+-------------+
|  101| Miller| 50000|2020-02-21|  M|pune|   101 Miller|
|  102|  Blake| 60000|2019-04-15|  M| hyd|    102 Blake|
|  103|  Priya| 30000|2021-05-22|  F|pune|    103 Priya|
|  104|   Sony| 70000|2018-05-22|  F| hyd|     104 Sony|
+-----+-------+------+----------+---+----+-------------+

iv)Generating new column based on condition
   ex:  Generate column Grade
                 if sal>=70000   ------------>Grade  "A"
                 if sal>=50000 and sal<70000->Grade  "B"
                 else                   ----->Grade  "C"


>>> from pyspark.sql.functions import when,lit
>>> df.withColumn('Grade', \
...  when((df.Salary >=70000),lit("A")) \
...  .when((df.Salary <70000)&(df.Salary >=50000),lit("B")).otherwise(lit("C"))).show() 
+-----+-------+------+----------+---+----+-----+
|Empid|Empnmae|Salary|  JoinDate|Sex|City|Grade|
+-----+-------+------+----------+---+----+-----+
|  101| Miller| 50000|2020-02-21|  M|pune|    B|
|  102|  Blake| 60000|2019-04-15|  M| hyd|    B|
|  103|  Priya| 30000|2021-05-22|  F|pune|    C|
|  104|   Sony| 70000|2018-05-22|  F| hyd|    A|
+-----+-------+------+----------+---+----+-----+
-----------------------------------------------------------------------------------
lit(): lit() function is used to add constant value as a new column

i)lit() function with select()

>>> from pyspark.sql.functions import col,lit
>>> df2=df.select(col("Empid"),col("Empname"),col("Salary"),lit("IBM").alias("Company"))
>>> df2.show()
+-----+-------+------+-------+
|Empid|Empname|Salary|Company|
+-----+-------+------+-------+
|  101| Miller| 50000|    IBM|
|  102|  Blake| 60000|    IBM|
|  103|  Priya| 30000|    IBM|
|  104|   Sony| 70000|    IBM|
+-----+-------+------+-------+

ii)lit() function with withColumn()

>>> df3=df.withColumn("Loan_status",when(col("Salary")>=55000,lit("Eligible")).otherwise(lit("Not Eligible")))
>>> df3.show()
+-----+-------+------+----------+---+----+------------+
|Empid|Empname|Salary|  JoinDate|Sex|City| Loan_status|
+-----+-------+------+----------+---+----+------------+
|  101| Miller| 50000|2020-02-21|  M|pune|Not Eligible|
|  102|  Blake| 60000|2019-04-15|  M| hyd|    Eligible|
|  103|  Priya| 30000|2021-05-22|  F|pune|Not Eligible|
|  104|   Sony| 70000|2018-05-22|  F| hyd|    Eligible|
+-----+-------+------+----------+---+----+------------+

-----------------------------------------------------------------------------------
withColumnRenamed()
using this function we can perform the following
i)to rename a column
ii)to rename multiple columns
iii)Dynamically rename all or multiple columns

syntax:
     withColumnRenamed("oldcolname","newcolname")

>>> df5=df.withColumnRenamed("Empid","Ecode")
>>> df5.show()
+-----+-------+------+----------+---+----+
|Ecode|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

ii)renaming multiple columns
   rename---->Salary--->Income
              Sex------>Gender

>>> df6=df.withColumnRenamed("Salary","Income") \
...  .withColumnRenamed("sex","Gender")
>>> df6.show()
+-----+-------+------+----------+------+----+
|Empid|Empname|Income|  JoinDate|Gender|City|
+-----+-------+------+----------+------+----+
|  101| Miller| 50000|2020-02-21|     M|pune|
|  102|  Blake| 60000|2019-04-15|     M| hyd|
|  103|  Priya| 30000|2021-05-22|     F|pune|
|  104|   Sony| 70000|2018-05-22|     F| hyd|
+-----+-------+------+----------+------+----+

iii)to change all the columns in a DF

>>> newcolumns=["col1","col2","col3","col4","col5","col5"]
>>> newcolumns=["col1","col2","col3","col4","col5","col6"]
>>> df7=df6.toDF(*newcolumns)
>>> df7.printSchema()
root
 |-- col1: long (nullable = true)
 |-- col2: string (nullable = true)
 |-- col3: long (nullable = true)
 |-- col4: string (nullable = true)
 |-- col5: string (nullable = true)
 |-- col6: string (nullable = true)

-----------------------------------------------------------------------------------
filter():

we can perform the following
1)Filter with column condition
>>> #Fiter those whose salaries>50000
... 
>>> df1=df.filter(df.Salary>50000)
>>> df1.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> #Filter only male emp records
... 
>>> df1=df.filter(df.Sex=='M')
>>> df1.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
+-----+-------+------+----------+---+----+


#Filter those emps  who belongs to other than "Hyd" city
>>> df2=df.filter(df.City != 'hyd')
>>> df2.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+

#Note:we can use comparision operators for filtering  > ,<,>=,<=,==,!=

2)Filtering using col() function

>>> #to retieve aparticular row
... 
>>> df3=df.filter(col("Empid")==104)
>>> df3.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

>>> #to retieve specific rows using col()
... 
>>> df4=df.filter(col("City")=="hyd")
>>> df4.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

3)Filtering with multiple conditions
>>> df6=df.filter((df.Salary>50000) & (df.City=="hyd"))
>>> df6.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+

4)filter based on list values
  isin() using this function , we can filter the elemens which are in list and
         which are not in list

Task:IF we have multiple cities, then filter only those emps who belongs to the 
     cities hyd,pune

>>> list1=["hyd","pune"]
>>> df7=df.filter(df.City.isin(list1))
>>> df7.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  102|  Blake| 60000|2019-04-15|  M| hyd|
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+


>>> #ex: other than hyd city
... 
>>> list1=["hyd"]
>>> df7=df.filter(df.City.isin(list1)==False)
>>> df7.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
|  103|  Priya| 30000|2021-05-22|  F|pune|
+-----+-------+------+----------+---+----+

Filter based on startswith,endswith
>>> df8=df.filter(df.Empname.startswith("M"))
>>> df8.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  101| Miller| 50000|2020-02-21|  M|pune|
+-----+-------+------+----------+---+----+

>>> df9=df.filter(df.JoinDate.endswith("22"))
>>> df9.show()
+-----+-------+------+----------+---+----+
|Empid|Empname|Salary|  JoinDate|Sex|City|
+-----+-------+------+----------+---+----+
|  103|  Priya| 30000|2021-05-22|  F|pune|
|  104|   Sony| 70000|2018-05-22|  F| hyd|
+-----+-------+------+----------+---+----+



----------------------------------------------------------------------------------
distinct():
>>> data=[("Ajay","mrkt",30000),
...       ("Miller","sales",40000),
...       ("Blake","Fin",50000),
...       ("Miller","sales",40000),
...       ("Ajay","mrkt",30000)]
>>> columns=["Ename","Dept","Salary"]
>>> df1=spark.createDataFrame(data,columns)
>>> df1.show()
+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
|Miller|sales| 40000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
|  Ajay| mrkt| 30000|
+------+-----+------+

>>> resDF=df1.distinct()
>>> resDF.show()

+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
+------+-----+------+

#I want the count of unique employees

>>> print("No of Employees=",resDF.count())
('No of Employees=', 3)

>>> print("No of Employees="+str(resDF.count()))
No of Employees=3

-----------------------------------------------------------------------------------
dropDuplicates():

>>> df2=df1.dropDuplicates()
>>> df2.show()

+------+-----+------+
| Ename| Dept|Salary|
+------+-----+------+
|  Ajay| mrkt| 30000|
| Blake|  Fin| 50000|
|Miller|sales| 40000|
+------+-----+------+

-----------------------------------------------------------------------------------