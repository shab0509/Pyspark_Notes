

#Submitting pyspark application:
-------------------------------

For creating spark application mainly we require driver

Driver--->which controls and drives the entire program
       -->it is a process(JVM)

spark---->implemented in python------>why again JVM required

by default spark built using scala--->derived from Java---->both uses JVM

so here programming in python,but spark built using scala i.e using JVM processes

----------------------------------------------------------------------------------
when we launch a spark application---->then 2 types of processes will be created.

1.Driver (only one driver process/one Application)
2.Executor (1 or more executors/one Application)

Spark Application runs on several worker nodes

driver process will be created by spark session class of pyspark.sql.module

the cluster we are using is YARN cluster

YARN--->distributed computing cluster----->comes along with hadoop

without hadoop---->There is no YARN.

in spark1 version-->to create driver, we use SparkContext class
in spark2 version-->to create driver,we use SparkSession

----------------------------------------------------------------------------------
syntax for spark-submit

spark-submit configurationoptions programpath i/ppath o/ppath

hadoop@ubuntu:~$ cat comment
spark is simple
spark is distributed
spark is for processing
spark is in-memory computing system


hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/comment
spark is simple
spark is distributed
spark is for processing
spark is in-memory computing system

Task: wordcount --->to count the number of occurences or each word in a file
program:wordcount.py

from pyspark import SparkConf,SparkContext
import sys
if(__name__=="__main__"):
   conf=SparkConf()
   sc=SparkContext('local','wordcountApp',conf=conf)
   r1=sc.textFile(sys.argv[1])
   r2=r1.flatMap(lambda x:x.split(' ')).map(lambda x:(x,1)).redu$
   r2.saveAsTextFile(sys.argv[2])
   sc.stop()

$ spark-submit /home/hadoop/wordcount.py /pyspark630pm/comment /pyspark630pm/wordcountres1

hadoop@ubuntu:~$ hdfs dfs -ls /pyspark630pm
Found 15 items
-rw-r--r--   1 hadoop supergroup         96 2024-01-07 04:44 /pyspark630pm/comment
drwxr-xr-x   - hadoop supergroup        
drwxr-xr-x   - hadoop supergroup          0 2024-03-23 07:52 /pyspark630pm/wordcountres1


hadoop@ubuntu:~$ hdfs dfs -ls /pyspark630pm/wordcountres1
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2024-03-23 07:52 /pyspark630pm/wordcountres1/_SUCCESS
-rw-r--r--   1 hadoop supergroup        142 2024-03-23 07:52 /pyspark630pm/wordcountres1/part-00000
hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/wordcountres1/part-00000
(u'in-memory', 1)
(u'for', 1)
(u'computing', 1)
(u'simple', 1)
(u'is', 4)
(u'processing', 1)
(u'distributed', 1)
(u'system', 1)
(u'spark', 4)

Spark Application can run in local/cluster(YARN)

local---->only one machine
cluster-->multiple nodes

here sc=sparkcontext('local'

if yarn cluster remove the local and place 'yarn'

i.e
sc=sparkContext('yarn')---->from spark2
sc=sparkContext('yarn-client')--->in spark1 version

----------------------------------------------------------------------------------
Q) How to configure the master ---------->(it can be (local/cluster(YARN))

from pyspark.sql import SparkSession

#creating spark deiver
spark=SparkSession.builder.master("local").appName("SampleApp1").getOrCreate()

here we are mentioning as local
if yarn cluster then say------->builder.master("yarn")
so while creating spark driver--->specify master as (local/yarn)

-----------------------------------------------------------------------------------
in the above example,we didnt specify about driver and executors
it takes default values and executes

$spark-submit --help


it displays various values

--executor-memory MEM -------------->Default value 1GB
                                     Default memory allocated to executor is 1GB.


what is the default memory allocated to driver

--driver-memory MEM         Memory for driver (Default: 1024M)------>1GB

default cores allocated are:
 --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,

-driver-cores NUM          Number of cores used by the driver, only in cluster mode
                              (Default: 1).

-----------------------------------------------------------------------------------

ex: 
   App1 ---------->utilizing
                   1GB 1Core    1GB 1core   1GB 1core
                   <--sys1-->   <--sys2-->  <--sys3-->

but each system has 8GB memory and 4cores
means 7GB memory and 3 cores free,which can be used for other Applications

ex:

  App2--------->utilizing
               2GB 2Core    2GB 2core   2GB 2core
               <--sys1-->   <--sys2-->  <--sys3-->
               still 5GB and 1 core is available for other Application
               now i can allocate 5GB and 1 core to App3

i.e we can distribute the resources(memory and cores)

in-order to allow multiple apps to run on a cluster
the application must be submitted to FAIR Scheduler
spark supports 2 schedulers------->FIFO/FAIR 

when we submit spark application in FIFO mode
ir wont allow 2nd application to be sunmitted untill 1st application is totally
executed means here only one application can run at a time
but to run multiple applications---->go for FAIR mode

-----------------------------------------------------------------------------------
configuring Driver Executor memory

spark-submit configurationoptions programpath i/ppath o/ppath

here configuration options is optional and here we can specify Executor and 
driver memory

I)
$spark-submit --executor-cores 2 --executor-memory 2G /home/hadoop/file1.py
 hdfs://localhost:9000/dirname/inputfilepath hdfs://localhost:9000/dirname/outputdir

in this output directory----->we get part-00000 file

II) submitting the application to the fair scheduler
$spark-submit --executor-cores 2 --executor-memory 2G --conf "spark.scheduler.mode"="FAIR" 
/home/hadoop/file1.py  hdfs://localhost:9000/dirname/inputfilepath 
hdfs://localhost:9000/dirname/outputdir

III)Driver memory and Driver cores
$spark-submit --executor-cores 2 --executor-memory 2G --conf "spark.scheduler.mode"="FAIR" 
--driver-cores 2 --driver-memory 2G /home/hadoop/file1.py  hdfs://localhost:9000/dirname/inputfilepath 
hdfs://localhost:9000/dirname/outputdir


--driver-cores 2------>these many cores not available for my system-->it gives error

we can add one more property
--deploy-mode DEPLOY_MODE

whether to launch the driver program locally("client) or on the worker machines
inside the cluster

Executors always be launched in worker node
 job is to process the data
 data will be within the worker nodes

Q)Driver can be launched in worker/outside of worker

a)If driver has to be launched in a worker node then deploy the applicaion in cluster mode only
b)f driver has to be launched out of worker nodes then deploy the application in client mode

when we submit the spark application from the master node, then the master node becomes the
client

client deploy mode:

the node from which  we submit the spark application to the cluster.If the driver is launched
in that node, then that is called as client deploy mode

Edge node or Gateway node

Gateway node--->gateway b/w the networks
A Node will be used b/w the cluster and the outside world.This node is called as Edge node

Instead of giving access to the entire cluster  an intermediate node
will be used , that will act as as interface b/w the cluster and outside world.

we can access the cluster through the edge node only, we cannot access the cluster directly
in producton

whatever we do,we should do through the edge node only

we submit applications theough the edge node

IF we want to see what is happening to the application within the cluster
who will maintain the entire information of the application---->Driver

executor--->only for running the app

if we want to run the driver on edge node---->deploy in client mode

if we want to run driver on one of the worker node--->deploy in cluster mode

if we wont specify any mode--->by default it is client mode

client mode---->for developing and testing

cluster mode--->for production

------------------------------------------------------------------------------------------------
submitting spark application to YARN

YARN--->master process is-------->Resource manager
        slave process is -------->Node manager

submitting spark application to YARN means ---->we are submitting spark application to

HDFS----->master process------>NameNode
          slave process------->DataNode

To communicate with HDFS(read/write),we need to interact with master process--->NameNode

we doesnt have direct interaction with slave processes which performs processing

ex: in a company, got a new project, then there wont be any direct interaction with the
    developers,where they execute and develop the code.
    instead project manager ,teamleader etc will be interacted

 -First data goes to them(project manager) and then data gets distributed to developers

-----------------------------------------------------------------------------------------------
$nano App1.py

from pyspark.sql import SparkSession
import sys

if __name__=="__main__":
    sparkdriver=SparkSession.builder.getOrCreate()
    df1=sparkdriver.read.format('csv').load(sys.argv[1])
    sparkdriver.stop()


here we didnt pass the appname and master---->i will pass while submitting--->i,e configuring options

$spark-submit --master local --executor-memory 2G --executor-cores 2 --deploy-mode client
/home/hadoop/App1.py  hdfs://localhost:9000/pyspatk630pm/sample1.csv













































































































































































































