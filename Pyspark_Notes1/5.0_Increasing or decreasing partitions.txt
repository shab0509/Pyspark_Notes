Increasing the number of partitions while parallelizing

>>> x=[10,20,30,40,50,60,70,80]
>>> r1=sc.parallelize(x)
>>> r1.getNumPartitions()
1
>>> #I want to increase the partitions while parallelizing
... 
>>> r2=sc.parallelize(x,2)
>>> r2.getNumPartitions()
2
>>> r2.saveAsTextFile("hdfs://localhost:9000/pyspark630pm/res2")

hadoop@ubuntu:~$ hdfs dfs -ls /pyspark630pm/res2
Found 3 items
-rw-r--r--   1 hadoop supergroup          0 2024-01-06 05:34 /pyspark630pm/res2/_SUCCESS
-rw-r--r--   1 hadoop supergroup         12 2024-01-06 05:34 /pyspark630pm/res2/part-00000
-rw-r--r--   1 hadoop supergroup         12 2024-01-06 05:34 /pyspark630pm/res2/part-00001

hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/res2/part-00001
50
60
70
80
hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/res2/part-00000
10
20
30
40

>>> r2.count()
8
>>> r2.getNumPartitions()
2

but i doesnt want 2 partitions,make it into one partition and one file.

for making into one partition,we have coalesce() function, which is a transformation function.

coalesce(): It is a Transformation to decrease or combine the partitions of a RDD.

>>> r2.getNumPartitions()
2
>>> r3=r2.coalesce(1)
>>> r3.getNumPartitions()
1
>>> r2.getNumPartitions()
2

can i increase the partitions using coalesce---??--->No
>>> r4=r2.coalesce(4)
>>> r4.getNumPartitions()
2
using coalesce,we can only decrease but we cannot increase the partitions

hadoop@ubuntu:~$ hdfs dfs -ls /pyspark630pm/res3
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2024-01-06 05:51 /pyspark630pm/res3/_SUCCESS
-rw-r--r--   1 hadoop supergroup         24 2024-01-06 05:51 /pyspark630pm/res3/part-00000

-----------------------------------------------------------------------------------------------
Increasing the number of partitions :2 ways
1.while parallelizing ,we need to specify the number of partitions we want
2.while loading a file from hdfs, we can specify the number of partitions we want

hadoop@ubuntu:~$ cat emp1.txt
101,Miller,10000,m,11
102,Blake,20000,m,12
103,Sony,30000,f,11
104,Sita,40000,f,12
105,James,50000,m,13hadoop@ubuntu:~$ 
hadoop@ubuntu:~$ hdfs dfs -put emp1.txt /pyspark630pm

Now Creating a RDD:
-whenever we load a file from hdfs using sparkcontext(sc) then a RDD is created

>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt")
>>> r1.getNumPartitions()
1
>>> #To increase the number of partitions then
... 
>>> r2=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt",3)
>>> r2.getNumPartitions()
3
>>> 



















