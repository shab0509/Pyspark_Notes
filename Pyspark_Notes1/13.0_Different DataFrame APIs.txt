Different API's of DataFrame:
-----------------------------

hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/emp1.txt
101,Miller,10000,m,11
102,Blake,20000,m,12
103,Sony,30000,f,11
104,Sita,40000,f,12
105,James,50000,m,13


>>> #creating a rdd
... 
>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt")
>>> r1.collect()
[u'101,Miller,10000,m,11', u'102,Blake,20000,m,12', u'103,Sony,30000,f,11', u'104,Sita,40000,f,12', u'105,James,50000,m,13']
>>> r1.getNumPartitions()
1
>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt",2)
>>> r1.getNumPartitions()
2
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[[u'101', u'Miller', u'10000', u'm', u'11'], [u'102', u'Blake', u'20000', u'm', u'12'], [u'103', u'Sony', u'30000', u'f', u'11'], [u'104', u'Sita', u'40000', u'f', u'12'], [u'105', u'James', u'50000', u'm', u'13']]
>>> #Each element to be converted into row type to create a DF
... 
>>> r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))
>>> r3.collect()
[Row(dno=11, eid=101, ename=u'Miller', sal=10000, sex=u'm'), Row(dno=12, eid=102, ename=u'Blake', sal=20000, sex=u'm'), Row(dno=11, eid=103, ename=u'Sony', sal=30000, sex=u'f'), Row(dno=12, eid=104, ename=u'Sita', sal=40000, sex=u'f'), Row(dno=13, eid=105, ename=u'James', sal=50000, sex=u'm')]
>>> df=spark.createDataFrame(r3)
>>> df.show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 11|101|Miller|10000|  m|
| 12|102| Blake|20000|  m|
| 11|103|  Sony|30000|  f|
| 12|104|  Sita|40000|  f|
| 13|105| James|50000|  m|
+---+---+------+-----+---+

-----------------------------------------------------------------------------------------------
1)select() : To select or extract a particular column

>>> # I want only ename column to be selected
... 
>>> e1=df.select("ename")
>>> #here e1 is also a  DF
... 
>>> e1.show()
+------+
| ename|
+------+
|Miller|
| Blake|
|  Sony|
|  Sita|
| James|
+------+
Transformation on a DF------->returns a DF
here df---------->has 5fields
e1 is also a dataframe-->with one field only


---------------------------------------------------------------------------------------------
2) To extract multiple columns

>>> df.select("ename","sal").show()
+------+-----+
| ename|  sal|
+------+-----+
|Miller|10000|
| Blake|20000|
|  Sony|30000|
|  Sita|40000|
| James|50000|
+------+-----+

----------------------------------------------------------------------------------------------
3)Transformations 
  Adding 3000 to each employee as salary

whenever we perform arithmetic operations, we need to use the Dataframe seperately
to each column

>>> df2=df.select(df.ename,df.sal+3000)
>>> df2.show()
+------+------------+
| ename|(sal + 3000)|
+------+------------+
|Miller|       13000|
| Blake|       23000|
|  Sony|       33000|
|  Sita|       43000|
| James|       53000|
+------+------------+

-----------------------------------------------------------------------------------------------
4) selectExpr(): for performing arithmetic operations on fields within a df

>>> df.selectExpr("ename","sal+3000").show()
+------+------------+
| ename|(sal + 3000)|
+------+------------+
|Miller|       13000|
| Blake|       23000|
|  Sony|       33000|
|  Sita|       43000|
| James|       53000|
+------+------------+
----------------------------------------------------------------------------------------------
5)filter()

Filter only those emps whose sal>20000

>>> df3=df.filter(df.sal>20000)
>>> df3.show()
+---+---+-----+-----+---+
|dno|eid|ename|  sal|sex|
+---+---+-----+-----+---+
| 11|103| Sony|30000|  f|
| 12|104| Sita|40000|  f|
| 13|105|James|50000|  m|
+---+---+-----+-----+---+

ex: sal>20000 and i want only 2 columns to be displayed--->ename,sal

selected rows and columns
>>> df3=df.filter(df.sal>20000).select("ename","sal")
>>> df3.show()
+-----+-----+
|ename|  sal|
+-----+-----+
| Sony|30000|
| Sita|40000|
|James|50000|
+-----+-----+

>>> df3=df.select("ename","sal").filter(df.sal>20000)
>>> df3.show()
+-----+-----+
|ename|  sal|
+-----+-----+
| Sony|30000|
| Sita|40000|
|James|50000|
+-----+-----+


#I want dno 11 emps only--->their names and salaries

>>> df4=df.filter(df.dno==11).select("ename","sal")
>>> df4.show()
+------+-----+
| ename|  sal|
+------+-----+
|Miller|10000|
|  Sony|30000|
+------+-----+

-----------------------------------------------------------------------------------------------
6)collect()
>>> df.collect()
[Row(dno=11, eid=101, ename=u'Miller', sal=10000, sex=u'm'), Row(dno=12, eid=102, ename=u'Blake', sal=20000, sex=u'm'), Row(dno=11, eid=103, ename=u'Sony', sal=30000, sex=u'f'), Row(dno=12, eid=104, ename=u'Sita', sal=40000, sex=u'f'), Row(dno=13, eid=105, ename=u'James', sal=50000, sex=u'm')]

-----------------------------------------------------------------------------------------------
7)count() :Returns the number of rows in a Dataframe

>>> df.count()
5
----------------------------------------------------------------------------------------------
8)columns :returns column names as list

>>> df.columns
['dno', 'eid', 'ename', 'sal', 'sex']

-----------------------------------------------------------------------------------------------
9)printSchema():
>>> df.printSchema()
root
 |-- dno: long (nullable = true)
 |-- eid: long (nullable = true)
 |-- ename: string (nullable = true)
 |-- sal: long (nullable = true)
 |-- sex: string (nullable = true)

-----------------------------------------------------------------------------------------------
10)describe()

>>> df.describe().show()
+-------+------------------+------------------+-----+------------------+----+
|summary|               dno|               eid|ename|               sal| sex|
+-------+------------------+------------------+-----+------------------+----+
|  count|                 5|                 5|    5|                 5|   5|
|   mean|              11.8|             103.0| null|           30000.0|null|
| stddev|0.8366600265340753|1.5811388300841898| null|15811.388300841896|null|
|    min|                11|               101|Blake|             10000|   f|
|    max|                13|               105| Sony|             50000|   m|
+-------+------------------+------------------+-----+------------------+----+

----------------------------------------------------------------------------------------------
11)distinct() :returns a new DF containg distinct rows

ex:
I want to know the number of deptnos
i.e  I want to know the distinct dnos

>>> df6=df.select("dno")
>>> df6.show()
+---+
|dno|
+---+
| 11|
| 12|
| 11|
| 12|
| 13|
+---+

>>> df6.distinct().show()
+---+
|dno|
+---+
| 12|
| 11|
| 13|
+---+

(or)

>>> df.select("dno").distinct().show()

+---+
|dno|
+---+
| 12|
| 11|
| 13|
+---+

-----------------------------------------------------------------------------------------------
12)orderBy():

>>>df.orderBy("dno").show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 11|101|Miller|10000|  m|
| 11|103|  Sony|30000|  f|
| 12|102| Blake|20000|  m|
| 12|104|  Sita|40000|  f|
| 13|105| James|50000|  m|
+---+---+------+-----+---+

>>> df.orderBy("ename").show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 12|102| Blake|20000|  m|
| 13|105| James|50000|  m|
| 11|101|Miller|10000|  m|
| 12|104|  Sita|40000|  f|
| 11|103|  Sony|30000|  f|
+---+---+------+-----+--

-----------------------------------------------------------------------------------------------
distinct() and orderBy():
>>> df6.distinct().orderBy("dno").show()

+---+
|dno|
+---+
| 11|
| 12|
| 13|
+---+
---------------------------------------------------------------------------------------------
13.sort():

>>> df.sort(df.sal).show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 11|101|Miller|10000|  m|
| 12|102| Blake|20000|  m|
| 11|103|  Sony|30000|  f|
| 12|104|  Sita|40000|  f|
| 13|105| James|50000|  m|
+---+---+------+-----+---+

>>> df.sort(df.sal.desc()).show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 13|105| James|50000|  m|
| 12|104|  Sita|40000|  f|
| 11|103|  Sony|30000|  f|
| 12|102| Blake|20000|  m|
| 11|101|Miller|10000|  m|
+---+---+------+-----+---+


---------------------------------------------------------------------------------------------
14.drop() :drops a particular column


>>> df5=df.drop("dno")
>>> df5.show()
+---+------+-----+---+
|eid| ename|  sal|sex|
+---+------+-----+---+
|101|Miller|10000|  m|
|102| Blake|20000|  m|
|103|  Sony|30000|  f|
|104|  Sita|40000|  f|
|105| James|50000|  m|
+---+------+-----+---+
we can also drop multiple columns at a time

>>> df6=df.drop("dno","sex")
>>> df6.show()
+---+------+-----+
|eid| ename|  sal|
+---+------+-----+
|101|Miller|10000|
|102| Blake|20000|
|103|  Sony|30000|
|104|  Sita|40000|
|105| James|50000|
+---+------+-----+
#-----------------------------------------------------------------------------
15.dropDuplicates():  drops duplicates based on multiple columns
ex:
   11  m
   11  m
   11  m
o/p:
   11 m
>>> df7=df.dropDuplicates(["dno","sex"])
>>> df7.show()

+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 12|104|  Sita|40000|  f|
| 12|102| Blake|20000|  m|
| 11|103|  Sony|30000|  f|
| 13|105| James|50000|  m|
| 11|101|Miller|10000|  m|
+---+---+------+-----+---+

#----------------------------------------------------------------------------
16.first() :returns the 1st row in the dataframe

>>> df.first()
Row(dno=11, eid=101, ename=u'Miller', sal=10000, sex=u'm')

------------------------------------------------------------------------------
17.Converting DF to a RDD
rdd1=df.rdd    
here rdd1 is the rdd which is created
------------------------------------------------------------------------------
18.
>>> df.rdd.getNumPartitions()
1

-----------------------------------------------------------------------------
19.replace():

>>> df2=df.replace(['m','f'],['male','female'],'sex')
>>> df2.show()
+---+---+------+-----+------+
|dno|eid| ename|  sal|   sex|
+---+---+------+-----+------+
| 11|101|Miller|10000|  male|
| 12|102| Blake|20000|  male|
| 11|103|  Sony|30000|female|
| 12|104|  Sita|40000|female|
| 13|105| James|50000|  male|
+---+---+------+-----+------+

ex:2
>>> df2=df.replace(['Miller','Blake'],['Ajay','Smith'],'ename')
>>> df2.show()
+---+---+-----+-----+---+
|dno|eid|ename|  sal|sex|
+---+---+-----+-----+---+
| 11|101| Ajay|10000|  m|
| 12|102|Smith|20000|  m|
| 11|103| Sony|30000|  f|
| 12|104| Sita|40000|  f|
| 13|105|James|50000|  m|
+---+---+-----+-----+---+
------------------------------------------------------------------------------
20) to change schema or the column names
eid--->ecode
sal--->income
sex--->Gender

>>> df2=df.toDF('dno','ecode','ename','income','Gender')
>>> df2.show()
+---+-----+-----+------+------+
|dno|ecode|ename|income|Gender|
+---+-----+-----+------+------+
| 11|  101| Ajay| 10000|     m|
| 12|  102|Smith| 20000|     m|
| 11|  103| Sony| 30000|     f|
| 12|  104| Sita| 40000|     f|
| 13|  105|James| 50000|     m|
+---+-----+-----+------+------+
------------------------------------------------------------------------------
21)withColumnRenamed(existing,new) : Renaming a particular column

>>> df.withColumnRenamed("ename","empname").show()
+---+---+-------+-----+---+
|dno|eid|empname|  sal|sex|
+---+---+-------+-----+---+
| 11|101|   Ajay|10000|  m|
| 12|102|  Smith|20000|  m|
| 11|103|   Sony|30000|  f|
| 12|104|   Sita|40000|  f|
| 13|105|  James|50000|  m|
+---+---+-------+-----+---+

--------------------------------------------------------------------------------
22)withColumn() :Adding a new column

>>> df2=df.withColumn('tax',df.sal*0.10)
>>> df2.show()
+---+---+-----+-----+---+------+
|dno|eid|ename|  sal|sex|   tax|
+---+---+-----+-----+---+------+
| 11|101| Ajay|10000|  m|1000.0|
| 12|102|Smith|20000|  m|2000.0|
| 11|103| Sony|30000|  f|3000.0|
| 12|104| Sita|40000|  f|4000.0|
| 13|105|James|50000|  m|5000.0|
+---+---+-----+-----+---+------+


>>> df2=df2.withColumn('netsal',df2.sal-df2.tax)
>>> df2.show()
+---+---+-----+-----+---+------+-------+
|dno|eid|ename|  sal|sex|   tax| netsal|
+---+---+-----+-----+---+------+-------+
| 11|101| Ajay|10000|  m|1000.0| 9000.0|
| 12|102|Smith|20000|  m|2000.0|18000.0|
| 11|103| Sony|30000|  f|3000.0|27000.0|
| 12|104| Sita|40000|  f|4000.0|36000.0|
| 13|105|James|50000|  m|5000.0|45000.0|
+---+---+-----+-----+---+------+-------+
-----------------------------------------------------------------------------------------------
23) toJSON()

>>> df.toJSON().collect()
[u'{"dno":11,"eid":101,"ename":"Miller","sal":10000,"sex":"m"}', 
u'{"dno":12,"eid":102,"ename":"Blake","sal":20000,"sex":"m"}', 
u'{"dno":11,"eid":103,"ename":"Sony","sal":30000,"sex":"f"}',
 u'{"dno":12,"eid":104,"ename":"Sita","sal":40000,"sex":"f"}', 
u'{"dno":13,"eid":105,"ename":"James","sal":50000,"sex":"m"}']

-----------------------------------------------------------------------------------------------
24)toLocalIterator():returns local python iterator object such as list,tuple,set,
                     and dict.


>>> df.toLocalIterator()
<itertools.chain object at 0x7fb41755f450>
>>> l1=list(df.toLocalIterator())
>>> print(l1)
[Row(dno=11, eid=101, ename=u'Miller', sal=10000, sex=u'm'), Row(dno=12, eid=102, ename=u'Blake', sal=20000, sex=u'm'), Row(dno=11, eid=103, ename=u'Sony', sal=30000, sex=u'f'), Row(dno=12, eid=104, ename=u'Sita', sal=40000, sex=u'f'), Row(dno=13, eid=105, ename=u'James', sal=50000, sex=u'm')]

-----------------------------------------------------------------------------------
25)groupBy()
#Task: select sex,count(*) from emp group by sex

>>> res1=df.groupBy("sex").count()
>>> res1.show()

+---+-----+    
|sex|count|
+---+-----+
|  m|    3|
|  f|    2|
+---+-----+

----------------------------------------------------------------------------------
26)Aggregated functions:
i)agg()
ii)sum()
iii)max()
iv)min()
v)avg()
vi)count()

case 1: Single grouping and single Aggregation
o/p:
m--->count
f--->count
>>> df.groupBy("sex").count().show()
o/p:

+---+-----+    
|sex|count|
+---+-----+
|  m|    3|
|  f|    2|
+---+-----+
#same task using agg()

>>> from pyspark.sql.functions import count
>>> df.groupBy("sex").agg(count("*")).show()


-----------------------------------------------------------------------------------
ii)sum aggregation
o/p:
m--->totsal
f--->totsal

>>> df.groupBy("sex").sum("sal").show()
+---+--------+ 
|sex|sum(sal)|
+---+--------+
|  m|   80000|
|  f|   70000|
+---+--------+

-----------------------------------------------------------------------------------
iii)avg

>>> df.groupBy("sex").avg("sal").show()

+---+------------------+
|sex|          avg(sal)|
+---+------------------+
|  m|26666.666666666668|
|  f|           35000.0|
+---+------------------+

-----------------------------------------------------------------------------------
iv)max
>>> df.groupBy("dno").max("sal").show()

+---+--------+ 
|dno|max(sal)|
+---+--------+
| 12|   40000|
| 11|   30000|
| 13|   50000|
+---+--------+

-----------------------------------------------------------------------------------
v)min:

>>> df.groupBy("dno").min("sal").show()

+---+--------+ 
|dno|min(sal)|
+---+--------+
| 12|   20000|
| 11|   10000|
| 13|   50000|
+---+--------+

----------------------------------------------------------------------------------
27)case 2: MultiGroupings:

>>> res1=df.groupBy("dno","sex").sum("sal").show()

+---+---+--------+
|dno|sex|sum(sal)|
+---+---+--------+
| 12|  f|   40000|
| 12|  m|   20000|
| 11|  f|   30000|
| 13|  m|   50000|
| 11|  m|   10000|
+---+---+--------+

#groupBy() and orderBy()
>>> res1=df.groupBy("dno","sex").sum("sal").orderBy("dno").show()

+---+---+--------+
|dno|sex|sum(sal)|
+---+---+--------+
| 11|  f|   30000|
| 11|  m|   10000|
| 12|  f|   40000|
| 12|  m|   20000|
| 13|  m|   50000|
+---+---+--------+
-----------------------------------------------------------------------------------
28) Multiple aggregations:

>>> from pyspark.sql.functions import count,sum,avg,max,min

>>> df.groupBy("sex").agg(sum("sal"),avg("sal"),max("sal"),min("sal"),count("*")).show()

+---+--------+------------------+--------+--------+--------+
|sex|sum(sal)|          avg(sal)|max(sal)|min(sal)|count(1)|
+---+--------+------------------+--------+--------+--------+
|  m|   80000|26666.666666666668|   50000|   10000|       3|
|  f|   70000|           35000.0|   40000|   30000|       2|
+---+--------+------------------+--------+--------+--------

#specifying the columnnames
>>> df.groupBy("sex").agg(sum("sal").alias("totsal"),avg("sal").alias("avgsal"),max("sal").alias("maxsal"),min("sal").alias("minsal"),count("*").alias("count")).show()

+---+------+------------------+------+------+-----+
|sex|totsal|            avgsal|maxsal|minsal|count|
+---+------+------------------+------+------+-----+
|  m| 80000|26666.666666666668| 50000| 10000|    3|
|  f| 70000|           35000.0| 40000| 30000|    2|
+---+------+------------------+------+------+-----+
-----------------------------------------------------------------------------------
29)union(): merging the rows of 2 DFS

The merging DF should have same schema

>>> df.union(df).show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 11|101|Miller|10000|  m|
| 12|102| Blake|20000|  m|
| 11|103|  Sony|30000|  f|
| 12|104|  Sita|40000|  f|
| 13|105| James|50000|  m|
| 11|101|Miller|10000|  m|
| 12|102| Blake|20000|  m|
| 11|103|  Sony|30000|  f|
| 12|104|  Sita|40000|  f|
| 13|105| James|50000|  m|
+---+---+------+-----+---+

df1.union(df2).union(df3).show()

-----------------------------------------------------------------------------------
30.intersect():the common rows will be returned
>>> df.intersect(df).show()

+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 11|101|Miller|10000|  m|
| 11|103|  Sony|30000|  f|
| 13|105| James|50000|  m|
| 12|104|  Sita|40000|  f|
| 12|102| Blake|20000|  m|
+---+---+------+-----+---+

------------------------------------------------------------------------------------
31.joins:used to collect data from two or more datasets

Horizontal merging :merging cols horizontally
ex:joins

There are 2 types of joins
1)inner join
2)Outer join---------->3types---->1)left outer join
                                  2)Right outer join
                                  3)Full outer join

A=1  B=1
  2    2
  3    3
  4    7
  5    8
  6    9

1)Inner join :only the matchings
 o/p:  (1,1)
       (2,2)
       (3,3)
2)Left outer join :Matchings +unmatched of left side i.e total presence of leftside
 o/p:
       (1,1)
       (2,2)
       (3,3)
       (4, )
       (5, )
       (6, )

3)Right outer join:Matchings +unmatched of right side i.e total presence of righttside
 o/p:
       (1,1)
       (2,2)
       (3,3)
       ( ,7)
       ( ,8)
       ( ,9)

4)Full outer join :Matchings +unmatched of left side i.e total presence of leftside
                             +unmatched of right side i.e total presence of rightside
 o/p:
       (1,1)
       (2,2)
       (3,3)
       (4, )
       (5, )
       (6, )
       ( ,7)
       ( ,8)
       ( ,9)

hadoop@ubuntu:~$ hdfs dfs -put emps1 /pyspark630pm
hadoop@ubuntu:~$ hdfs dfs -put dept1 /pyspark630pm
hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/emps1
101,aaa,1000,m,11
102,bbb,2000,f,12
103,ccc,3000,m,12
104,ddd,4000,f,13
105,eee,5000,m,11
106,fff,6000,f,14
107,ggg,7000,m,15
108,hhh,8000,f,16hadoop@ubuntu:~$ 
hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/dept1
11,mrkt,hyd
12,HR,delhi
13,fin,pune
17,HR,hyd
18,fin,pune
19,mrkt,delhi

>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emps1")
>>> r1.collect()
[Stage 0:>                                                                                                                        [u'101,aaa,1000,m,11', u'102,bbb,2000,f,12', u'103,ccc,3000,m,12', u'104,ddd,4000,f,13', u'105,eee,5000,m,11', u'106,fff,6000,f,14', u'107,ggg,7000,m,15', u'108,hhh,8000,f,16']
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[Stage 1:>                                                                                                                        [[u'101', u'aaa', u'1000', u'm', u'11'], [u'102', u'bbb', u'2000', u'f', u'12'], [u'103', u'ccc', u'3000', u'm', u'12'], [u'104', u'ddd', u'4000', u'f', u'13'], [u'105', u'eee', u'5000', u'm', u'11'], [u'106', u'fff', u'6000', u'f', u'14'], [u'107', u'ggg', u'7000', u'm', u'15'], [u'108', u'hhh', u'8000', u'f', u'16']]
>>> from pyspark.sql import Row
>>> r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))
>>> r3.collect()
[Row(dno=11, eid=101, ename=u'aaa', sal=1000, sex=u'm'), Row(dno=12, eid=102, ename=u'bbb', sal=2000, sex=u'f'), Row(dno=12, eid=103, ename=u'ccc', sal=3000, sex=u'm'), Row(dno=13, eid=104, ename=u'ddd', sal=4000, sex=u'f'), Row(dno=11, eid=105, ename=u'eee', sal=5000, sex=u'm'), Row(dno=14, eid=106, ename=u'fff', sal=6000, sex=u'f'), Row(dno=15, eid=107, ename=u'ggg', sal=7000, sex=u'm'), Row(dno=16, eid=108, ename=u'hhh', sal=8000, sex=u'f')]
>>> df1=spark.createDataFrame(r3)
2024-02-10 06:12:05 WARN  ObjectStore:568 - Failed to get database global_temp, returning NoSuchObjectException
>>> df1.show()
+---+---+-----+----+---+
|dno|eid|ename| sal|sex|
+---+---+-----+----+---+
| 11|101|  aaa|1000|  m|
| 12|102|  bbb|2000|  f|
| 12|103|  ccc|3000|  m|
| 13|104|  ddd|4000|  f|
| 11|105|  eee|5000|  m|
| 14|106|  fff|6000|  f|
| 15|107|  ggg|7000|  m|
| 16|108|  hhh|8000|  f|
+---+---+-----+----+---+

>>> rr1=sc.textFile("hdfs://localhost:9000/pyspark630pm/dept1")
>>> rr1.collect()
[u'11,mrkt,hyd', u'12,HR,delhi', u'13,fin,pune', u'17,HR,hyd', u'18,fin,pune', u'19,mrkt,delhi']
>>> rr2=rr1.map(lambda x:x.split(","))
>>> rr2.collect()
[[u'11', u'mrkt', u'hyd'], [u'12', u'HR', u'delhi'], [u'13', u'fin', u'pune'], [u'17', u'HR', u'hyd'], [u'18', u'fin', u'pune'], [u'19', u'mrkt', u'delhi']]
>>> rr3=rr2.map(lambda x:Row(dno=int(x[0]),dname=x[1],city=x[2]))
>>> rr3.collect()
[Row(city=u'hyd', dname=u'mrkt', dno=11), Row(city=u'delhi', dname=u'HR', dno=12), Row(city=u'pune', dname=u'fin', dno=13), Row(city=u'hyd', dname=u'HR', dno=17), Row(city=u'pune', dname=u'fin', dno=18), Row(city=u'delhi', dname=u'mrkt', dno=19)]
>>> dept_df=spark.createDataFrame(rr3)
>>> dept_df.show()
+-----+-----+---+
| city|dname|dno|
+-----+-----+---+
|  hyd| mrkt| 11|
|delhi|   HR| 12|
| pune|  fin| 13|
|  hyd|   HR| 17|
| pune|  fin| 18|
|delhi| mrkt| 19|
+-----+-----+---+

syntax for join:
df1.join(df2,joining condition,"type of Join")

types:
1.inner
2.outer
3.left_outer
4.right_outer
5.full_outer

>>> ij=df1.join(dept_df,df1.dno==dept_df.dno,"inner").select(df1.ename,
df1.eid,df1.sal,df1.sex,df1.dno,dept_df.dname,dept_df.city)
>>> ij.show()

+-----+---+----+---+---+-----+-----+
|ename|eid| sal|sex|dno|dname| city|
+-----+---+----+---+---+-----+-----+
|  bbb|102|2000|  f| 12|   HR|delhi|
|  ccc|103|3000|  m| 12|   HR|delhi|
|  aaa|101|1000|  m| 11| mrkt|  hyd|
|  eee|105|5000|  m| 11| mrkt|  hyd|
|  ddd|104|4000|  f| 13|  fin| pune|
+-----+---+----+---+---+-----+-----+
--------------------------------------------------------------------------
2)left outer join:

>>> loj=df1.join(dept_df,df1.dno==dept_df.dno,"left_outer").
select(df1.ename,df1.eid,df1.sal,df1.sex,df1.dno,dept_df.dname,
dept_df.city)
>>> loj.show()
                     
+-----+---+----+---+---+-----+-----+
|ename|eid| sal|sex|dno|dname| city|
+-----+---+----+---+---+-----+-----+
|  bbb|102|2000|  f| 12|   HR|delhi|
|  ccc|103|3000|  m| 12|   HR|delhi|
|  aaa|101|1000|  m| 11| mrkt|  hyd|
|  eee|105|5000|  m| 11| mrkt|  hyd|
|  ddd|104|4000|  f| 13|  fin| pune|
|  fff|106|6000|  f| 14| null| null|
|  ggg|107|7000|  m| 15| null| null|
|  hhh|108|8000|  f| 16| null| null|
-----------------------------------

3)right outer join:
>>> roj=df1.join(dept_df,df1.dno==dept_df.dno,"right_outer").select(df1.ename,df1.eid,df1.sal,df1.sex,df1.dno,dept_df.dname,dept_df.city)
>>> roj.show()

+-----+----+----+----+----+-----+-----+
|ename| eid| sal| sex| dno|dname| city|
+-----+----+----+----+----+-----+-----+
| null|null|null|null|null| mrkt|delhi|
| null|null|null|null|null|   HR|  hyd|
|  bbb| 102|2000|   f|  12|   HR|delhi|
|  ccc| 103|3000|   m|  12|   HR|delhi|
|  aaa| 101|1000|   m|  11| mrkt|  hyd|
|  eee| 105|5000|   m|  11| mrkt|  hyd|
|  ddd| 104|4000|   f|  13|  fin| pune|
| null|null|null|null|null|  fin| pune|
+-----+----+----+----+----+-----+-----+

-------------------------------------------------------------------------
4)full outer join:

>>> foj=df1.join(dept_df,df1.dno==dept_df.dno,"full_outer").
select(df1.ename,df1.eid,df1.sal,df1.sex,df1.dno,dept_df.dname,
dept_df.city)
>>> foj.show()

+-----+----+----+----+----+-----+-----+
|ename| eid| sal| sex| dno|dname| city|
+-----+----+----+----+----+-----+-----+
| null|null|null|null|null| mrkt|delhi|
| null|null|null|null|null|   HR|  hyd|
|  bbb| 102|2000|   f|  12|   HR|delhi|
|  ccc| 103|3000|   m|  12|   HR|delhi|
|  aaa| 101|1000|   m|  11| mrkt|  hyd|
|  eee| 105|5000|   m|  11| mrkt|  hyd|
|  ddd| 104|4000|   f|  13|  fin| pune|
| null|null|null|null|null|  fin| pune|
|  fff| 106|6000|   f|  14| null| null|
|  ggg| 107|7000|   m|  15| null| null|
|  hhh| 108|8000|   f|  16| null| null|
+-----+----+----+----+----+-----+-----+

-------------------------------------------------------------------------
32) Eliminating the records with null values

>>> foj.dropna().show()

+-----+---+----+---+---+-----+-----+
|ename|eid| sal|sex|dno|dname| city|
+-----+---+----+---+---+-----+-----+
|  bbb|102|2000|  f| 12|   HR|delhi|
|  ccc|103|3000|  m| 12|   HR|delhi|
|  aaa|101|1000|  m| 11| mrkt|  hyd|
|  eee|105|5000|  m| 11| mrkt|  hyd|
|  ddd|104|4000|  f| 13|  fin| pune|
+-----+---+----+---+---+-----+-----+
-------------------------------------------------------------------------------------
33) nulls of columns with numerical values replace with 0

>>> foj.na.fill(0).show()

+-----+---+----+----+---+-----+-----+
|ename|eid| sal| sex|dno|dname| city|
+-----+---+----+----+---+-----+-----+
| null|  0|   0|null|  0| mrkt|delhi|
| null|  0|   0|null|  0|   HR|  hyd|
|  bbb|102|2000|   f| 12|   HR|delhi|
|  ccc|103|3000|   m| 12|   HR|delhi|
|  aaa|101|1000|   m| 11| mrkt|  hyd|
|  eee|105|5000|   m| 11| mrkt|  hyd|
|  ddd|104|4000|   f| 13|  fin| pune|
| null|  0|   0|null|  0|  fin| pune|
|  fff|106|6000|   f| 14| null| null|
|  ggg|107|7000|   m| 15| null| null|
|  hhh|108|8000|   f| 16| null| null|
+-----+---+----+----+---+-----+-----+

-----------------------------------------------------------------------------------
34) replace numerical columns having nulls---->with 0
    replace string columns having nulls ------>with "unknown"

>>> foj.na.fill({'ename':'unknown','eid':0,'sal':0,'sex':'unkown','dno':0,
'dname':'unknown','city':'unknown'}).show()

+-------+---+----+------+---+-------+-------+
|  ename|eid| sal|   sex|dno|  dname|   city|
+-------+---+----+------+---+-------+-------+
|unknown|  0|   0|unkown|  0|   mrkt|  delhi|
|unknown|  0|   0|unkown|  0|     HR|    hyd|
|    bbb|102|2000|     f| 12|     HR|  delhi|
|    ccc|103|3000|     m| 12|     HR|  delhi|
|    aaa|101|1000|     m| 11|   mrkt|    hyd|
|    eee|105|5000|     m| 11|   mrkt|    hyd|
|    ddd|104|4000|     f| 13|    fin|   pune|
|unknown|  0|   0|unkown|  0|    fin|   pune|
|    fff|106|6000|     f| 14|unknown|unknown|
|    ggg|107|7000|     m| 15|unknown|unknown|
|    hhh|108|8000|     f| 16|unknown|unknown|
+-------+---+----+------+---+-------+-------+















