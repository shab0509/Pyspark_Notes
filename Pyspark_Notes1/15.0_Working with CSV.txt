

working with CSV(Comma Seperated Values)
----------------------------------------

hadoop@ubuntu:~$ cat emp1.csv
eid,ename,sal,sex,dno,city
101,Miller,40000,m,11,hyd
102,Blake,50000,m,12,pune
103,Sony,60000,f,13,pune
104,Sita,70000,f,12,hyd
105,James,80000,m,11,hyd
hadoop@ubuntu:~$ hdfs dfs -put emp1.csv /pyspark630pm

I-method
--------
>>> df=spark.read.format("csv").load("hdfs://localhost:9000/pyspark630pm/emp1.csv")
>>> df.show()  
+---+------+-----+---+---+----+
|_c0|   _c1|  _c2|_c3|_c4| _c5|
+---+------+-----+---+---+----+
|eid| ename|  sal|sex|dno|city|
|101|Miller|40000|  m| 11| hyd|
|102| Blake|50000|  m| 12|pune|
|103|  Sony|60000|  f| 13|pune|
|104|  Sita|70000|  f| 12| hyd|
|105| James|80000|  m| 11| hyd|
+---+------+-----+---+---+----+

II-method:
----------

>>> df=spark.read.csv("hdfs://localhost:9000/pyspark630pm/emp1.csv")
>>> df.show()
+---+------+-----+---+---+----+
|_c0|   _c1|  _c2|_c3|_c4| _c5|
+---+------+-----+---+---+----+
|eid| ename|  sal|sex|dno|city|
|101|Miller|40000|  m| 11| hyd|
|102| Blake|50000|  m| 12|pune|
|103|  Sony|60000|  f| 13|pune|
|104|  Sita|70000|  f| 12| hyd|
|105| James|80000|  m| 11| hyd|
+---+------+-----+---+---+----+

----------------------------------------------------------------------------------
3)To set header names

>>> df2=spark.read.option("header",True).csv("hdfs://localhost:9000/pyspark630pm/emp1.csv")
>>> df2.show()
+---+------+-----+---+---+----+
|eid| ename|  sal|sex|dno|city|
+---+------+-----+---+---+----+
|101|Miller|40000|  m| 11| hyd|
|102| Blake|50000|  m| 12|pune|
|103|  Sony|60000|  f| 13|pune|
|104|  Sita|70000|  f| 12| hyd|
|105| James|80000|  m| 11| hyd|
+---+------+-----+---+---+----+

-----------------------------------------------------------------------------------
4)Reading multiple CSV files at a time
ex: To read 3 csv files ar a time

syntax : df=spark.read.csv(["csvfilepath1","csvfilepath2","csvfilepath3"])

>>> df3=spark.read.csv(["hdfs://localhost:9000/pyspark630pm/emp1.csv","hdfs://localhost:9000/pyspark630pm/emp1.csv","hdfs://localhost:9000/pyspark630pm/emp1.csv"])
>>> df3.show()
+---+------+-----+---+---+----+
|_c0|   _c1|  _c2|_c3|_c4| _c5|
+---+------+-----+---+---+----+
|eid| ename|  sal|sex|dno|city|
|101|Miller|40000|  m| 11| hyd|
|102| Blake|50000|  m| 12|pune|
|103|  Sony|60000|  f| 13|pune|
|104|  Sita|70000|  f| 12| hyd|
|105| James|80000|  m| 11| hyd|
|eid| ename|  sal|sex|dno|city|
|101|Miller|40000|  m| 11| hyd|
|102| Blake|50000|  m| 12|pune|
|103|  Sony|60000|  f| 13|pune|
|104|  Sita|70000|  f| 12| hyd|
|105| James|80000|  m| 11| hyd|
|eid| ename|  sal|sex|dno|city|
|101|Miller|40000|  m| 11| hyd|
|102| Blake|50000|  m| 12|pune|
|103|  Sony|60000|  f| 13|pune|
|104|  Sita|70000|  f| 12| hyd|
|105| James|80000|  m| 11| hyd|
+---+------+-----+---+---+----+

-----------------------------------------------------------------------------------
5)Reading all the csv files in a directory(assume 10 files)
syntax: df=spark.read.csv("folderpath")

create a new HDFS firectory(CSV) and store all the csv files in it
hadoop@ubuntu:~$ hdfs dfs -mkdir /pyspark630pm/csv

hadoop@ubuntu:~$ cat emp1.csv > emp2.csv
hadoop@ubuntu:~$ cat emp1.csv > emp3.csv
hadoop@ubuntu:~$ nano emp3.csv and remove 2 columns from this

hadoop@ubuntu:~$ cat emp3.csv
eid,ename,sal
101,Miller,40000
102,Blake,50000
103,Sony,60000
104,Sita,70000
105,James,80000

hadoop@ubuntu:~$ hdfs dfs -put emp1.csv emp2.csv emp3.csv /pyspark630pm/csv 

hadoop@ubuntu:~$ hdfs dfs -ls /pyspark630pm/csv
Found 3 items
-rw-r--r--   1 hadoop supergroup        153 2024-02-17 05:42 /pyspark630pm/csv/emp1.csv
-rw-r--r--   1 hadoop supergroup        153 2024-02-17 05:42 /pyspark630pm/csv/emp2.csv
-rw-r--r--   1 hadoop supergroup         93 2024-02-17 05:42 /pyspark630pm/csv/emp3.csv

Now loading the entire folder and creating a dataframe from it

>>> df=spark.read.csv("hdfs://localhost:9000/pyspark630pm/csv")
>>> 
>>> 
>>> df.show()
+---+------+-----+----+----+----+
|_c0|   _c1|  _c2| _c3| _c4| _c5|
+---+------+-----+----+----+----+
|eid| ename|  sal| sex| dno|city|
|101|Miller|40000|   m|  11| hyd|
|102| Blake|50000|   m|  12|pune|
|103|  Sony|60000|   f|  13|pune|
|104|  Sita|70000|   f|  12| hyd|
|105| James|80000|   m|  11| hyd|
|eid| ename|  sal| sex| dno|city|
|101|Miller|40000|   m|  11| hyd|
|102| Blake|50000|   m|  12|pune|
|103|  Sony|60000|   f|  13|pune|
|104|  Sita|70000|   f|  12| hyd|
|105| James|80000|   m|  11| hyd|
|eid| ename|  sal|null|null|null|
|101|Miller|40000|null|null|null|
|102| Blake|50000|null|null|null|
|103|  Sony|60000|null|null|null|
|104|  Sita|70000|null|null|null|
|105| James|80000|null|null|null|
+---+------+-----+----+----+----+

#specifying a delimiter

-by default, it is comma,but we can set to other delimiter like "\t" ,"\\"

syntax :df=spark.read.options(delimiter=',').csv("filepath")










