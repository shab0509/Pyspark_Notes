Loading data and filtering:
---------------------------

>>> #Task : Filter those employees whose sal>20000
... 
>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt")
>>> r1.collect()
[u'101,Miller,10000,m,11', u'102,Blake,20000,m,12', u'103,Sony,30000,f,11', u'104,Sita,40000,f,12', u'105,James,50000,m,13']
>>> #step 2: splitting each record--->we get a list
... 
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[[u'101', u'Miller', u'10000', u'm', u'11'], [u'102', u'Blake', u'20000', u'm', u'12'], [u'103', u'Sony', u'30000', u'f', u'11'], [u'104', u'Sita', u'40000', u'f', u'12'], [u'105', u'James', u'50000', u'm', u'13']]
>>> #step 3:Extract only names and salaries
... 
>>> r3=r2.map(lambda x:(x[1],int(x[2])))
>>> r3.collect()
[(u'Miller', 10000), (u'Blake', 20000), (u'Sony', 30000), (u'Sita', 40000), (u'James', 50000)]
>>> #step 4: filtering
... 
>>> r4=r3.filter(lambda x:x[1]>20000)
>>> r4.collect()
[(u'Sony', 30000), (u'Sita', 40000), (u'James', 50000)]
>>> 

------------------------------------------------------------------------------------------------
ex:2

Getting records of a particular department(11)

>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt")
>>> r1.collect()
[Stage 0:>                                                          (0 + 1) /                                                                               [u'101,Miller,10000,m,11', u'102,Blake,20000,m,12', u'103,Sony,30000,f,11', u'104,Sita,40000,f,12', u'105,James,50000,m,13']
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[Stage 1:>                                                          (0 + 1) /                                                                               [[u'101', u'Miller', u'10000', u'm', u'11'], [u'102', u'Blake', u'20000', u'm', u'12'], [u'103', u'Sony', u'30000', u'f', u'11'], [u'104', u'Sita', u'40000', u'f', u'12'], [u'105', u'James', u'50000', u'm', u'13']]
>>> #filterig based on condition

>>> r3=r2.filter(lambda x:x[4]=="11")
>>> r3.collect()
[[u'101', u'Miller', u'10000', u'm', u'11'], [u'103', u'Sony', u'30000', u'f', u'11']]


ex:3 applying multiple conditions
>>> r4=r2.map(lambda x:(x[1],int(x[2]),int(x[4])))
>>> r4.collect()
[(u'Miller', 10000, 11), (u'Blake', 20000, 12), (u'Sony', 30000, 11), (u'Sita', 40000, 12), (u'James', 50000, 13)]
>>> r5=r4.filter(lambda x:x[2]==11 and x[1]>20000)
>>> r5.collect()
[(u'Sony', 30000, 11)]

----------------------------------------------------------------------------------------------

ex:4 wordcount: To count the number of occurences of each word in a file.

hadoop@ubuntu:~$ cat comment
spark is simple
spark is distributed
spark is for processing
spark is in-memory computing systemhadoop@ubuntu:~$
 
hadoop@ubuntu:~$ hdfs dfs -put comment /pyspark630pm

>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/comment")
>>> r1.collect()
[u'spark is simple', u'spark is distributed', u'spark is for processing', u'spark is in-memory computing system']
>>> #splitting each record ,we get list of words
... 
>>> rdd_words=r1.map(lambda x:x.split(" "))
>>> rdd_words.collect()
[[u'spark', u'is', u'simple'], [u'spark', u'is', u'distributed'], [u'spark', u'is', u'for', u'processing'], [u'spark', u'is', u'in-memory', u'computing', u'system']]
>>> #Now flattenning the list iinto a single list
... 
>>> words1=rdd_words.flatMap(lambda x:x)
>>> words1.collect()
[u'spark', u'is', u'simple', u'spark', u'is', u'distributed', u'spark', u'is', u'for', u'processing', u'spark', u'is', u'in-memory', u'computing', u'system']
>>> #or combining the above 2 steps 
... 
>>> words1=r1.flatMap(lambda x:x.split(" "))
>>> words1.collect()
[u'spark', u'is', u'simple', u'spark', u'is', u'distributed', u'spark', u'is', u'for', u'processing', u'spark', u'is', u'in-memory', u'computing', u'system']
>>> #To apply reduceByKey,we require (k,v) pair
... 
>>> #so add 1 to each word as key and form a (k,v) pair
... 
>>> pair=words1.map(lambda x:(x,1))
>>> pair.collect()
[(u'spark', 1), (u'is', 1), (u'simple', 1), (u'spark', 1), (u'is', 1), (u'distributed', 1), (u'spark', 1), (u'is', 1), (u'for', 1), (u'processing', 1), (u'spark', 1), (u'is', 1), (u'in-memory', 1), (u'computing', 1), (u'system', 1)]
>>> res=pair.reduceByKey(lambda x,y:x+y)
>>> res.collect()
[(u'in-memory', 1), (u'for', 1), (u'computing', 1), (u'simple', 1), (u'is', 4), 
(u'processing', 1), (u'distributed', 1), (u'system', 1), (u'spark', 4)]

#I want the count of only spark word

>>> res2=res.filter(lambda x:x[0]=='spark')
>>> res2.collect()
[(u'spark', 4)]
 






















