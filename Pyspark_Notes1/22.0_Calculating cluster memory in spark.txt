

How to calculate the cluster memory in spark while executing spark job.

for parallelism---->we run the code in different machines instead of running in a single machine

spark 1.xxx-------->sparkcontext
spark 2.xxx-------->spark session

-when we submit a spark job then sparkcontext or sparksession initiates a JVM process
 to execute the entire job. This JVM process is called as executor.

-1Node---->can have 1 or more executors
 again inside the executor,we have to mention,how many cores we have

here cores means threads

Executor is a JVM process,we need some threads to execute the tasks,these execution threads
are called as cores.

Q)What is the ideal or optimal count of cores for each executor??----->4

-How to see the No of cores in a machine---->TaskManager---->performance tab--->No of cores :4

 here cores means threads

calculating the cluster memory in spark while executing spark job

ex: consider 4node cluster
    i.e 4machines

   Node1             Node2                 Node3                  Node4
 4cores for Executor 4cores for Executor   4cores for Executor    4cores for Executor
 64GB Memory         64GB Memeory          64GB Memory            64GB Memory
 16cores             16cores               16 cores               16cores

Calcultations:

Cluster Nodes     :4
Each Node Memory  :64G
Each has cores    :16

why we have to calculate this,why spark cannot do this???

Spark does it with some default or dynamic configuration

ex:
    submiting spark jobs

    job1 ---->Tester testing sum of 2 no's
              takes less time---->wont require more memory

    job2----->some Database operations
              takes more time
              require more memory

    job3 ---->join or Group by operation
              here we have lot of shuffle operations b/w the executors
              requires lot of memory
              takes lot of time to release the resources

    job4
    job5 --->these also takes more time and memory

   If we leave this to spark--->it does but with default configurations

Calcultations:

Cluster Nodes     :4
Each Node Memory  :64G
Each has cores    :16

Leave one core for other background processes(left out cores are)------>15
Total cores available in all 4 nodes----------------------------------->15x4=60 cores
                                                                           (Total available cores)

Number of Executors required=totalcores/no of cores per executor-------->60/4=15 executors

Leave one Executor for Application manager  ---------------------------->15 -1 =14 executors
(bcoz-->once we start the execution,we need to sendback
         the results (or) others)

Executors per Node---------------------------------------------------->14/4nodes=3.5 ~4 

Memory per Executor ---------------------------------------->(64-1(i.e 1GB for other processes)
                                                            =63
                                                            =63/executors per Node
                                                          =63/4 ~ 16 =16GB/Executor we have to leave


6 to 10% of Executor memory we need to leave for overhead memory
i.e overhead errors
ex: available----->10GB
    but the processes required 11GB--->this is memory overhead error
                                   ---->memory overflow exception


so for resolving these,we usually leave memory for overhead

6 to 10% is the typical memory we leave
ex:8%------------->16x(8/100) = 1.28 ~ 2GB for overhead memory
                here 16 is memory per executor

that means  16-2GB=14GB is the executor memory

previously we said 4 executors/node
                   63/4=16-2 =14GB


Finally,
    Node 1
    -4 cores per Executor
    -64 GB Memory
    -16cores
    -4Executors /Node
    -14GB/Executor
    -2GB for overhead'

These are the basic calculations we do,before we submit the spark job

spark.executor.cores=4      (4cores/Executor)
spark.executor.instances=4  (4 Executors/node)
spark.executor.memory=14G
spark.yarn.executor.memoryoverhead=2g
spark.driver.memory=1g ------------->default value
                            or change to 2g

spark submit parameters are

spark-submit --master yarn \
             --driver-memory 1G \
             --num-executors 4 \
             --executor-cores 4 \
             --executor-memory 14G \
             --i/p filepath \
             --o/p filepath

#or

conf=sparkConf()
conf.set("spark.driver.memory","1g")

-----------------------------------------------------------------------------------------------


























 

























 















