

Reading different file formats in pyspark:
1)CSV
2)JSON
3)parquet
4)ORC
5)AVRO

1)CSV file

>>> from pyspark.sql import SparkSession
>>> spark=SparkSession.builder.appName("DemoApp").getOrCreate()
>>> filepath="hdfs://localhost:9000/pyspark630pm/emp1.csv"
>>> 
>>> #Reading a csvfile and creating a Dataframe
... 
>>> df=spark.read.format('csv').option("inferschema","true").option("Header","true").option("delimiter",",").load(filepath)

>>> df.show()
+---+------+------+---+---+----+
|eid| ename|salary|sex|dno|city|
+---+------+------+---+---+----+
|101|Miller| 40000|  m| 11|null|
|102| Blake| 50000|  m| 12|pune|
|103|  Sony|  null|  f| 11|null|
|104|  Sita| 70000|  f| 12| hyd|
|105|  John|  null|  m| 13| hyd|
+---+------+------+---+---+----+

#writing into a CSV file( creating a csv from a df)
>>> df.write.option("header","true").csv("hdfs://localhost:9000/pyspark630pm/csv1/")

hadoop@ubuntu:~$ hdfs dfs -ls /pyspark630pm/csv1
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2023-09-17 04:51 /pyspark4pm/csv1/_SUCCESS
-rw-r--r--   1 hadoop supergroup        154 2023-09-17 04:51 /pyspark4pm/csv1/part-00000-d94e8839-1ac6-44b5-82ac-6b5cf5ca98ed-c000.csv
hadoop@ubuntu:~$ hdfs dfs -cat /pyspark4pm/csv1/part-00000-d94e8839-1ac6-44b5-82ac-6b5cf5ca98ed-c000.csv
eid,ename,salary,sex,dno,city
101,Miller,40000,m,11,null
102,Blake,50000,m,12,pune
103,Sony,null,f,11,null
104,Sita,70000,f,12,hyd
105,John,null,m,13,hyd

-----------------------------------------------------------------------------------------------
2)JSON file

>>> from pyspark.sql import SparkSession
>>> spark=SparkSession.builder.appName("DemoApp").getOrCreate()
>>> filepath="hdfs://localhost:9000/pyspark4pm/json1"
>>> df=spark.read.format("json").load(filepath)
>>> df.show()
+---+----+------+---+
|age|city|  name|sex|
+---+----+------+---+
| 30|null|  Ajay|  m|
| 25|null|Miller|  m|
| 30|null| Latha|  f|
| 22| hyd|  Sony|  f|
| 33|pune|  John|  m|
+---+----+------+---+

#writing into a jsonfile (creating a JSON from a df)
>>> df.write.json("hdfs://localhost:9000/pyspark4pm/json5/")


hadoop@ubuntu:~$ hdfs dfs -ls /pyspark4pm/json5
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2023-09-17 05:06 /pyspark4pm/json5/_SUCCESS
-rw-r--r--   1 hadoop supergroup        205 2023-09-17 05:06 /pyspark4pm/json5/part-00000-e3181d6d-bcb2-4d97-8672-803670e21302-c000.json
hadoop@ubuntu:~$ hdfs dfs -cat /pyspark4pm/json5/part-00000-e3181d6d-bcb2-4d97-8672-803670e21302-c000.json
{"age":30,"name":"Ajay","sex":"m"}
{"age":25,"name":"Miller","sex":"m"}
{"age":30,"name":"Latha","sex":"f"}
{"age":22,"city":"hyd","name":"Sony","sex":"f"}
{"age":33,"city":"pune","name":"John","sex":"m"}

----------------------------------------------------------------------------------------------
3)parquet file:
  It is a column-oriented binary file format
  It is used for large-scale queries

>>> from pyspark.sql import SparkSession
>>> spark=SparkSession.builder.appName("DemoApp").getOrCreate()
>>> filepath="hdfs://localhost:9000/pyspark4pm/userdata.parquet"
>>> df=spark.read.format("parquet").load(filepath)
>>> df.show()

#writing into a parquet file(creating a parquetfile fram a df)
df.write.parquet("hdfs://localhost:9000/pyspark4pm/parquet/")

----------------------------------------------------------------------------------------------

4)ORC (optimized Row Columnar)
  

>>> from pyspark.sql import SparkSession
>>> spark=SparkSession.builder.appName("DemoApp").getOrCreate()
>>> filepath="hdfs://localhost:9000/pyspark4pm/sample1.orc"
>>> df=spark.read.format("orc").load(filepath)
>>> df.show()

#writing into a orc file(creating a orcfile fram a df)
df.write.orc("hdfs://localhost:9000/pyspark4pm/parquet/")

---------------------------------------------------------------------------------------------
5)AVRO file:
  
>>> from pyspark.sql import SparkSession
>>> spark=SparkSession.builder.appName("DemoApp").getOrCreate()
>>> filepath="hdfs://localhost:9000/pyspark4pm/sample1.avro"
>>> df=spark.read.format("avro").load(filepath)
>>> df.show()

#writing into a parquet file(creating a parquetfile from a df)
df.write.format("avro").save("hdfs://localhost:9000/pyspark4pm/avro/")





