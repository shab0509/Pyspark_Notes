

RDD operations :2 operations
1.Transformations
2.Actions

IF Transformations applied on a RDD---------->resultant is also a RDD(spark object)
IF actions applied on a RDD ------------------>resultant is a local object(python object)

converting a python object to spark object

IF we parallelize a python object------>we get a spark object(RDD)
ex:

>>> x=[10,20,30,40,50]
>>> rdd1=sc.parallelize(x)
>>> rdd1.collect()
o/p:
[10,20,30,40,50]

Various in-built Transformations:
1.map()
2.flatMap()
3.filter()
4.union()
5.intersection()
6.substract()
7.cartesian()
8.distinct()

1.map() :For each element Transformation

Applying operation to each element of a RDD and returns a RDD.

ex: squaring each element of a RDD

>>> r1=sc.parallelize([10,20,30,40,50])
>>> r1.collect()
[10, 20, 30, 40, 50]
>>> r2=r1.map(lambda x:x*x)
>>> r2.collect()
o/p:
[100, 400, 900, 1600, 2500]

----------------------------
ex:2 adding 5 to each element of a RDD
>>> r1.collect()
[10, 20, 30, 40, 50]
>>> r2=r1.map(lambda x:x+5)
>>> r2.collect()
[15, 25, 35, 45, 55]

----------------------------------------------------------------------------------------------
2. flatMap() :flattens the lists->multiple collections into one collection

>>> x=[[10,20,30],[40,50,60],[70,80,90]]
>>> y=sc.parallelize(x)    -->here y is an RDD 
>>> z=y.flatMap(lambda x:x) -->here z is an RDD(bcoz T/R on a RDD---->gives RDD)
>>> w=z.collect()           -->w is a python object as action is performed on z
>>>print(w)
[10, 20, 30, 40, 50, 60, 70, 80, 90]

---------------------------------------------------------------------------------------------
3.filter(): filter elements of a RDD based on condition and resultant also is a RDD

>>> sals=[10000,20000,30000,40000,50000]
>>> rdd1=sc.parallelize(sals)
>>> rdd2=rdd1.filter(lambda x:x>=30000)
>>> res=rdd2.collect()
>>> print(res)
[30000, 40000, 50000]

ex:2
>>> a=["python","Java","Devops","Spark","Scala"]
>>> #Filter other than Java
... 
>>> rdd1=sc.parallelize(a)
>>> rdd2=rdd1.filter(lambda x:x!="Java")
>>> res=rdd2.collect()
>>> print(res)
['python', 'Devops', 'Spark', 'Scala']

---------------------------------------------------------------------------------------------
4.union():combines elements of multiple RDDs
          by default it performs union all operation(allows duplicates)

>>> r1=sc.parallelize([10,20,30,40,50])
>>> r2=sc.parallelize([10,20,30,60,70])
>>> res=r1.union(r2)  here res is a RDD
>>> res2=res.collect()
>>> print(res2)
[10, 20, 30, 40, 50, 10, 20, 30, 60, 70]

ex:
>>> r1=sc.parallelize([[101,'Miller',10000],[102,'Blake',20000]])
>>> r2=sc.parallelize([[101,'Miller',10000],[103,'James',30000]])
>>> r3=r1.union(r2)
>>> res=r3.collect()
>>> print(res)
[[101, 'Miller', 10000], [102, 'Blake', 20000], [101, 'Miller', 10000], [103, 'James', 30000]]
>>> r1.count()
2
>>> r2.count()
2
>>> r3.count()
4
>>> r3.take(2)
[[101, 'Miller', 10000], [102, 'Blake', 20000]]


---------------------------------------------------------------------------------------------
5.intersection :returns only the common elements

>>> spark_stds=sc.parallelize(["Ajay","Rohith","Miller","Blake","James"])
>>> devops_stds=sc.parallelize(["Amar","Antony","Ajay","John","Rohith"])
>>> #find the students who are learning both
... 
>>> stds_learning_both=spark_stds.intersection(devops_stds)
>>> stds_learning_both.collect()
['Rohith', 'Ajay']

----------------------------------------------------------------------------------------------
6.substract()

>>> r1=sc.parallelize([10,20,30,40,50])
>>> r2=sc.parallelize([10,20,30,60,70])
>>> r3=r1.subtract(r2)
>>> r3.collect()
[40, 50]
>>> 
>>> r4=r2.subtract(r1)
>>> r4.collect()
[70, 60]

ex:2
>>> spark_stds=sc.parallelize(["Ajay","Rohith","Miller","Blake","James"])
>>> devops_stds=sc.parallelize(["Amar","Antony","Ajay","John","Rohith"])
>>> #Find only those who are learning only spark
... 
>>> spark_only=spark_stds.subtract(devops_stds)
>>> spark_only.collect()
['Blake', 'James', 'Miller']

------------------------------------------------------------------------------------------
7.cartesian(): Each element of left side will merge with each element of right side

>>> dnos=sc.parallelize([11,12,13])
>>> sals=sc.parallelize([10000,20000,30000])
>>> res=dnos.cartesian(sals)
>>> res2=res.collect()
>>> print(res2)
[(11, 10000), (11, 20000), (11, 30000), (12, 10000), (12, 20000), (12, 30000), (13, 10000), (13, 20000), (13, 30000)]

-----------------------------------------------------------------------------------------------
8.distinct() :Removes the duplicates

>> r1=sc.parallelize([10,20,30,40,10,20,30])
>>> res=r1.distinct()
>>> res.collect()
[40, 10, 20, 30]

applying multiple Transdormations----->distinct and filter
>>> r1=sc.parallelize([10,20,30,40,10,20,30])
>>> res=r1.distinct().filter(lambda x:x>20)   
>>> res.collect()
[40, 30]


----------------------------------------------------------------------------------------------
Transformations on a pair RDDs i.e (k,v) pairs

1.reduceByKey()
2.groupByKey()
3.sortByKey()
4.mapValues()
5.keys()
6.values()
7.join()
8.leftOuterJoin()
9.rightOuterJoin()
10.fullOuterJoin()

1.reduceByKey() :sum up all the values with same key
                 reduceByKey can be applied only on (k,v) pairs

>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res=r1.reduceByKey(lambda x,y:x+y)
>>> res.collect()
[(11, 60000), (12, 60000), (13, 90000)]


ex:
>>> medals=sc.parallelize([("Ind",1),("Eng",1),("Jap",1),("Ind",1),("Jap",1),("Ind",1),("Eng",1),("Ind",1)])
>>> res=medals.reduceByKey(lambda x,y:x+y)
>>> res.collect()
[('Ind', 4), ('Jap', 2), ('Eng', 2)]

-------------------------------------------------------------------------------------------------
2.groupByKey():

>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res=r1.groupByKey()
>>> res.collect()
[(11, <pyspark.resultiterable.ResultIterable object at 0x7f70e1efaf90>), (12, <pyspark.resultiterable.ResultIterable object at 0x7f70e1f06090>), (13, <pyspark.resultiterable.ResultIterable object at 0x7f70e1f06050>)]
>>> #here we got iterable object(compact buffer) convert it to list and access
>>> res2=res.map(lambda x:(x[0],list(x[1])))
>>> res2.collect()
[(11, [10000, 50000]), (12, [20000, 40000]), (13, [30000, 60000])]
>>> res3=res2.map(lambda x:(x[0],sum(x[1]),max(x[1]),min(x[1]),len(x[1]),sum(x[1])/len(x[1])))
>>> res3.collect()
[(11, 60000, 50000, 10000, 2, 30000), (12, 60000, 40000, 20000, 2, 30000), (13, 90000, 60000, 30000, 2, 45000)]
-----------------------------------------------------------------------------------------------
3.sortByKey(): sorting based on key

r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res=r1.sortByKey()
>>> res.collect()
[(11, 10000), (11, 50000), (12, 20000), (12, 40000), (13, 30000), (13, 60000)]

-----------------------------------------------------------------------------------------------
4.mapValues(): Applying a functionality to each value,without changing the key

>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res=r1.mapValues(lambda x:x+5000)
>>> res.collect()
[(11, 15000), (12, 25000), (13, 35000), (12, 45000), (11, 55000), (13, 65000)]

(or)using map()
>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res2=r1.map(lambda x:(x[0],x[1]+5000))
>>> res2.collect()
[(11, 15000), (12, 25000), (13, 35000), (12, 45000), (11, 55000), (13, 65000)]

-----------------------------------------------------------------------------------------------
5.keys():returns the keys of a RDD

>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res=r1.keys()
>>> res.collect()
[11, 12, 13, 12, 11, 13]
-----------------------------------------------------------------------------------------------
6.values():returns the values of a RDD
>>> r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,40000),(11,50000),(13,60000)])
>>> res=r1.values()
>>> res.collect()
[10000, 20000, 30000, 40000, 50000, 60000]
>>> x=res.collect()
>>> x.reverse()
>>> x
[60000, 50000, 40000, 30000, 20000, 10000]

---------------------------------------------------------------------------------------------
7.joins():

A=1  B=1
  2    2
  3    3
  4    7
  5    8
  6    9

1)Inner join :only the matchings
 o/p:  (1,1)
       (2,2)
       (3,3)
2)Left outer join :Matchings +unmatched of left side i.e total presence of leftside
 o/p:
       (1,1)
       (2,2)
       (3,3)
       (4, )
       (5, )
       (6, )

3)Right outer join:Matchings +unmatched of right side i.e total presence of righttside
 o/p:
       (1,1)
       (2,2)
       (3,3)
       ( ,7)
       ( ,8)
       ( ,9)

4)Full outer join :Matchings +unmatched of left side i.e total presence of leftside
                             +unmatched of right side i.e total presence of rightside
 o/p:
       (1,1)
       (2,2)
       (3,3)
       (4, )
       (5, )
       (6, )
       ( ,7)
       ( ,8)
       ( ,9)

join():

>>> r1=sc.parallelize([(10,20),(30,40),(50,60)])
>>> r2=sc.parallelize([(10,25),(30,45),(10,55),(70,80)])
>>> #1.inner join---->only matching tuples
... 
>>> ij_res=r1.join(r2)
>>> ij_res.collect()
[(10, (20, 25)), (10, (20, 55)), (30, (40, 45))]
>>> 
>>> #left outer join: MAtchings + unmatched of leftside
... 
>>> loj_res=r1.leftOuterJoin(r2)
>>> loj_res.collect()
[(10, (20, 25)), (10, (20, 55)), (50, (60, None)), (30, (40, 45))]
>>> #right outer join: MAtchings + unmatched of rightside
... 
>>> roj_res=r1.rightOuterJoin(r2)
>>> roj_res.collect()
[(10, (20, 25)), (10, (20, 55)), (70, (None, 80)), (30, (40, 45))]
>>> #full outer join: MAtchings + unmatched of leftside and rightside
... 
>>> foj_res=r1.fullOuterJoin(r2)
>>> foj_res.collect()
[(10, (20, 25)), (10, (20, 55)), (70, (None, 80)), (50, (60, None)), (30, (40, 45))]
>>> 
----------------------------------------------------------------------------------------------


















































