
Spark sql:

-Spark sql is used to process Spark data objects using SQL stmts

-Spark data objects are called as RDDs

-on top of RDDs,Dataframes are introduced.

-RDDs need to be converted as Dataframes and these Dataframes should be registered as tables
 to execute sql stmts

-Dataframes are rquivalent to temporatry tables of sql

-Spark sql follows Mysql standard sql.

-Spark Core provides spark context object.

-Spark sql provides SqlVontext object to perfom sql operations and RDD operations

-Spark Sql has a sub-component called sparkhql,wich provides a context object
 called hive context object,using this spark can be integrated with hive,so that from
 spark itself we can perform hql operations with in-memory conputing 

 Operations on  a RDD ----->sparkcontext
 operations on  a DF  ----->SqlContext
 operations on hive tables->HiveContext

----------------------------------------------------------------------------------------------
DataFrame is a collection of Row objects,each Row object represents a record

DF provides operations to SQL queries which is not present in RDD.

Creating DataFrames:

DF can be created in 4ways:

1)from RDDs
2)from local objects(python objects)
3)from results of queries
4)from External Datasources

-----------------------------------------------------------------------------------------------
1)Creating Dataframes from RDDs ------------>RDD.toDF

for converting local object or RDD into DF,we use the following method

spark.createDataFrame(List/RDD,schema)

1st parameter------>either List or RDD
2nd parameter ----->Column names

we can create DataFrame using
1)spark session ==>ex: df=spark.createDatFrame(RDD) #here spark is the spark session object
2)sqlContext    ==>ex: df=sqlContext.createDataFrame(RDD) #using sqlContext

-----------------------------------------------------------------------------------------------
4 Different ways of Providing Schema

1.while creating DF,providing schema

2.using dictionary

3.using Row object

4.using StructType

-----------------------------------------------------------------------------------------------

ex:1 Creating DataFrames from Lists (local objects)

>>> x=[('Miller',25),('Blake',30)]
>>> df=spark.createDataFrame(x)
>>> df.show()
+------+---+
|    _1| _2|
+------+---+
|Miller| 25|
| Blake| 30|
+------+---+
-----------------------------------------------------------------------------------------------
ex:2 Creating DF with schema

>>> x=[('Miller',25),('Blake',30),('James',50)]
>>> df=spark.createDataFrame(x,['NAME','AGE'])
>>> df.show()
+------+---+
|  NAME|AGE|
+------+---+
|Miller| 25|
| Blake| 30|
| James| 50|
+------+---+

-----------------------------------------------------------------------------------------------
3)df.count() : To count the number of rows in a DF.''

>>> df.count()
3

-----------------------------------------------------------------------------------------------
4)printSchema()

>>> df.printSchema()
root
 |-- NAME: string (nullable = true)
 |-- AGE: long (nullable = true)


-----------------------------------------------------------------------------------------------
5)USing Dictionary Providing Schema,Creating a DF

>>> x=[{'name':'Ajay','age':25},{'name':'Rahul','age':30}]
>>> df=spark.createDataFrame(x)
>>> df.show()
+---+-----+
|age| name|
+---+-----+
| 25| Ajay|
| 30|Rahul|
+---+-----+

ex:2
>>> x=[{'name':'Ajay','age':25},{'name':'Rahul','age':30,'city':'Pune'}]
>>> df=spark.createDataFrame(x)
>>> df.show()
+---+-----+----+
|age| name|city|
+---+-----+----+
| 25| Ajay|null|
| 30|Rahul|Pune|
+---+-----+----+


---------------------------------------------------------------------------------------------
6) Creating a DF from  a RDD

>>> x=[('Miller',25),('Blake',30),('James',50)]
>>> rdd=sc.parallelize(x)
>>> df1=spark.createDataFrame(rdd)
>>> df1.show()
+------+---+
|    _1| _2|
+------+---+
|Miller| 25|
| Blake| 30|
| James| 50|
+------+---+

Adding schema to a RDD

>>> df2=spark.createDataFrame(rdd,['Name','Age'])
>>> df2.show()
+------+---+
|  Name|Age|
+------+---+
|Miller| 25|
| Blake| 30|
| James| 50|
+------+---+

-----------------------------------------------------------------------------------------------
7)Creating a DF using Row object


>>> x=[('Miller',25),('Ajay',30),('Blake',45)]
>>> rdd=sc.parallelize(x)
>>> #Now providing schema to a RDD
... 
>>> from pyspark.sql import Row
>>> customer=Row('Name','Age')
>>> type(customer)
<class 'pyspark.sql.types.Row'>
>>> cust=rdd.map(lambda p:customer(*p))
>>> cust.collect()
[Row(Name='Miller', Age=25), Row(Name='Ajay', Age=30), Row(Name='Blake', Age=45)]
>>> df3=spark.createDataFrame(cust)
>>> df3.show()
+------+---+
|  Name|Age|
+------+---+
|Miller| 25|
|  Ajay| 30|
| Blake| 45|
+------+---+
-----------------------------------------------------------------------------------------------
8)providing schema using StructType

>>> from pyspark.sql.types import *
>>> schema=StructType([
... StructField("Name",StringType(),True),
... StructField("Age",IntegerType(),True)])
>>> df4=spark.createDataFrame(rdd,schema)
>>> df4.show()
+------+---+
|  Name|Age|
+------+---+
|Miller| 25|
|  Ajay| 30|
| Blake| 45|
+------+---+

------------------------------------------------------------------------------------------------
9)changing schema or column names

>>> df5=spark.createDataFrame(rdd,"Empname:string,Empage:int")
>>> df5.show()
+-------+------+
|Empname|Empage|
+-------+------+
| Miller|    25|
|   Ajay|    30|
|  Blake|    45|
+-------+------+

----------------------------------------------------------------------------------------------
10)
>>> df5.rdd.getNumPartitions()
1
>>> 
----------------------------------------------------------------------------------------------
11)creating emp records

>>> from pyspark.sql import Row

>>> x=[(101,'miller',10000,'m',11),
...    (102,'Blake',20000,'m',12),
...    (103,'Sony',30000,'f',12),
...    (104,'Sita',40000,'f',13),
...    (105,'James',50000,'m',12)]
>>> r1=sc.parallelize(x)
>>> df=spark.createDataFrame(r1,['eid','ename','sal','sex','dno'] )

>>> df.show()
+---+------+-----+---+---+
|eid| ename|  sal|sex|dno|
+---+------+-----+---+---+
|101|miller|10000|  m| 11|
|102| Blake|20000|  m| 12|
|103|  Sony|30000|  f| 12|
|104|  Sita|40000|  f| 13|
|105| James|50000|  m| 12|
+---+------+-----+---+---+

>>> df.collect()
[Row(eid=101, ename=u'miller', sal=10000, sex=u'm', dno=11), 
 Row(eid=102, ename=u'Blake', sal=20000, sex=u'm', dno=12), 
 Row(eid=103, ename=u'Sony', sal=30000, sex=u'f', dno=12), 
 Row(eid=104, ename=u'Sita', sal=40000, sex=u'f', dno=13), 
 Row(eid=105, ename=u'James', sal=50000, sex=u'm', dno=12)]

-----------------------------------------------------------------------------------------------
12) Loading data from HDFS and creating RDD and converting into DF

hadoop@ubuntu:~$ hdfs dfs -cat /pyspark630pm/emp1.txt
101,Miller,10000,m,11
102,Blake,20000,m,12
103,Sony,30000,f,11
104,Sita,40000,f,12
105,James,50000,m,13


>>> #creating a rdd
... 
>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt")
>>> r1.collect()
[u'101,Miller,10000,m,11', u'102,Blake,20000,m,12', u'103,Sony,30000,f,11', u'104,Sita,40000,f,12', u'105,James,50000,m,13']
>>> r1.getNumPartitions()
1
>>> r1=sc.textFile("hdfs://localhost:9000/pyspark630pm/emp1.txt",2)
>>> r1.getNumPartitions()
2
>>> r2=r1.map(lambda x:x.split(","))
>>> r2.collect()
[[u'101', u'Miller', u'10000', u'm', u'11'], [u'102', u'Blake', u'20000', u'm', u'12'], [u'103', u'Sony', u'30000', u'f', u'11'], [u'104', u'Sita', u'40000', u'f', u'12'], [u'105', u'James', u'50000', u'm', u'13']]
>>> #Each element to be converted into row type to create a DF
... 
>>> r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))
>>> r3.collect()
[Row(dno=11, eid=101, ename=u'Miller', sal=10000, sex=u'm'), Row(dno=12, eid=102, ename=u'Blake', sal=20000, sex=u'm'), Row(dno=11, eid=103, ename=u'Sony', sal=30000, sex=u'f'), Row(dno=12, eid=104, ename=u'Sita', sal=40000, sex=u'f'), Row(dno=13, eid=105, ename=u'James', sal=50000, sex=u'm')]
>>> df=spark.createDataFrame(r3)
>>> df.show()
+---+---+------+-----+---+
|dno|eid| ename|  sal|sex|
+---+---+------+-----+---+
| 11|101|Miller|10000|  m|
| 12|102| Blake|20000|  m|
| 11|103|  Sony|30000|  f|
| 12|104|  Sita|40000|  f|
| 13|105| James|50000|  m|
+---+---+------+-----+---+

















































































